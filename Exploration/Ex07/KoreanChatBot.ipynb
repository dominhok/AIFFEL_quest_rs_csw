{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "10-1. í”„ë¡œì íŠ¸: í•œêµ­ì–´ ë°ì´í„°ë¡œ ì±—ë´‡ ë§Œë“¤ê¸°"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 1 & 2 ë°ì´í„° ìˆ˜ì§‘í•˜ê¸° & ë°ì´í„° ì „ì²˜ë¦¬í•˜ê¸°"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Q</th>\n",
       "      <th>A</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>12ì‹œ ë•¡!</td>\n",
       "      <td>í•˜ë£¨ê°€ ë˜ ê°€ë„¤ìš”.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1ì§€ë§ í•™êµ ë–¨ì–´ì¡Œì–´</td>\n",
       "      <td>ìœ„ë¡œí•´ ë“œë¦½ë‹ˆë‹¤.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3ë°•4ì¼ ë†€ëŸ¬ê°€ê³  ì‹¶ë‹¤</td>\n",
       "      <td>ì—¬í–‰ì€ ì–¸ì œë‚˜ ì¢‹ì£ .</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3ë°•4ì¼ ì •ë„ ë†€ëŸ¬ê°€ê³  ì‹¶ë‹¤</td>\n",
       "      <td>ì—¬í–‰ì€ ì–¸ì œë‚˜ ì¢‹ì£ .</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>PPL ì‹¬í•˜ë„¤</td>\n",
       "      <td>ëˆˆì‚´ì´ ì°Œí‘¸ë ¤ì§€ì£ .</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Q            A  label\n",
       "0           12ì‹œ ë•¡!   í•˜ë£¨ê°€ ë˜ ê°€ë„¤ìš”.      0\n",
       "1      1ì§€ë§ í•™êµ ë–¨ì–´ì¡Œì–´    ìœ„ë¡œí•´ ë“œë¦½ë‹ˆë‹¤.      0\n",
       "2     3ë°•4ì¼ ë†€ëŸ¬ê°€ê³  ì‹¶ë‹¤  ì—¬í–‰ì€ ì–¸ì œë‚˜ ì¢‹ì£ .      0\n",
       "3  3ë°•4ì¼ ì •ë„ ë†€ëŸ¬ê°€ê³  ì‹¶ë‹¤  ì—¬í–‰ì€ ì–¸ì œë‚˜ ì¢‹ì£ .      0\n",
       "4          PPL ì‹¬í•˜ë„¤   ëˆˆì‚´ì´ ì°Œí‘¸ë ¤ì§€ì£ .      0"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('ChatbotData.csv')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11823"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ì¤‘ë³µëœ í—¤ë“œë¼ì¸: ê²°í˜¼ì´ë‚˜ í•˜ì§€ ì™œ ìê¾¸ ë‚˜í•œí…Œ í™” ë‚´ëƒêµ¬!\n",
      "ì¤‘ë³µëœ í–‰ì˜ ì¸ë±ìŠ¤: [152, 5527]\n",
      "\n",
      "ì¤‘ë³µëœ í—¤ë“œë¼ì¸: ê³ ë°±í•˜ê³  í›„íšŒí•˜ë©´ ì–´ë–¡í•˜ì§€\n",
      "ì¤‘ë³µëœ í–‰ì˜ ì¸ë±ìŠ¤: [189, 5537]\n",
      "\n",
      "ì¤‘ë³µëœ í—¤ë“œë¼ì¸: ê³ ì–‘ì´ í‚¤ìš°ê³  ì‹¶ì–´\n",
      "ì¤‘ë³µëœ í–‰ì˜ ì¸ë±ìŠ¤: [195, 196]\n",
      "\n",
      "ì¤‘ë³µëœ í—¤ë“œë¼ì¸: ê³µë¶€ëŠ” ë‚´ ì²´ì§ˆì´ ì•„ë‹Œ ê²ƒ ê°™ì•„\n",
      "ì¤‘ë³µëœ í–‰ì˜ ì¸ë±ìŠ¤: [226, 5542]\n",
      "\n",
      "ì¤‘ë³µëœ í—¤ë“œë¼ì¸: ê³µì‹œ ì¤€ë¹„ í˜ë“¤ì–´\n",
      "ì¤‘ë³µëœ í–‰ì˜ ì¸ë±ìŠ¤: [234, 235]\n",
      "\n",
      "ì¤‘ë³µëœ í—¤ë“œë¼ì¸: ê¸°ìˆ™ì‚¬ ê´œì°®ì„ê¹Œ\n",
      "ì¤‘ë³µëœ í–‰ì˜ ì¸ë±ìŠ¤: [377, 5704]\n",
      "\n",
      "ì¤‘ë³µëœ í—¤ë“œë¼ì¸: ë‚˜ëŠ” ì¢‹ì€ë° â€¦.\n",
      "ì¤‘ë³µëœ í–‰ì˜ ì¸ë±ìŠ¤: [592, 5774]\n",
      "\n",
      "ì¤‘ë³µëœ í—¤ë“œë¼ì¸: ë‚¨ë™ìƒí•œí…Œ ìê¾¸ í™”ë‚´ê²Œ ë˜ë„¤\n",
      "ì¤‘ë³µëœ í–‰ì˜ ì¸ë±ìŠ¤: [703, 5828]\n",
      "\n",
      "ì¤‘ë³µëœ í—¤ë“œë¼ì¸: ë‚¨ìì¸ì§€ ì—¬ìì¸ì§€ ì•Œë ¤ì¤˜\n",
      "ì¤‘ë³µëœ í–‰ì˜ ì¸ë±ìŠ¤: [720, 5839]\n",
      "\n",
      "ì¤‘ë³µëœ í—¤ë“œë¼ì¸: ë‚­ë§Œì´ë¼ê³ ëŠ” ì—†ì–´ê°€ì§€êµ¬\n",
      "ì¤‘ë³µëœ í–‰ì˜ ì¸ë±ìŠ¤: [782, 5848]\n",
      "\n",
      "ì¤‘ë³µëœ í—¤ë“œë¼ì¸: ë‚´ ì´ë¦„ì´ ì—†ì–´\n",
      "ì¤‘ë³µëœ í–‰ì˜ ì¸ë±ìŠ¤: [808, 5865]\n",
      "\n",
      "ì¤‘ë³µëœ í—¤ë“œë¼ì¸: ë‚´ê°€ ë­˜ ì¢‹ì•„í•˜ëŠ”ì§€ ì˜í•˜ëŠ”ì§€ ëª¨ë¥´ê² ì–´\n",
      "ì¤‘ë³µëœ í–‰ì˜ ì¸ë±ìŠ¤: [842, 5875]\n",
      "\n",
      "ì¤‘ë³µëœ í—¤ë“œë¼ì¸: ë‚´ì¼ ë§Œë‚˜ìê³  í•´ë³¼ê¹Œ?\n",
      "ì¤‘ë³µëœ í–‰ì˜ ì¸ë±ìŠ¤: [890, 9541]\n",
      "\n",
      "ì¤‘ë³µëœ í—¤ë“œë¼ì¸: ë„ˆ ë§Œë“  ì‚¬ëŒì€ ëˆ„êµ¬ì•¼?\n",
      "ì¤‘ë³µëœ í–‰ì˜ ì¸ë±ìŠ¤: [931, 5901]\n",
      "\n",
      "ì¤‘ë³µëœ í—¤ë“œë¼ì¸: ë„ˆë¬´ í˜ë“¤ë‹¤\n",
      "ì¤‘ë³µëœ í–‰ì˜ ì¸ë±ìŠ¤: [980, 5944, 5945]\n",
      "\n",
      "ì¤‘ë³µëœ í—¤ë“œë¼ì¸: ë„ˆë¬´í•˜ë„¤ ì§„ì§œ\n",
      "ì¤‘ë³µëœ í–‰ì˜ ì¸ë±ìŠ¤: [982, 5963]\n",
      "\n",
      "ì¤‘ë³µëœ í—¤ë“œë¼ì¸: ë„˜ ë§ì´ ë¨¹ì—ˆë‹¤.\n",
      "ì¤‘ë³µëœ í–‰ì˜ ì¸ë±ìŠ¤: [985, 5982]\n",
      "\n",
      "ì¤‘ë³µëœ í—¤ë“œë¼ì¸: ë†€ì•„ ì¤„ ì‚¬ëŒì´ ì—†ì–´\n",
      "ì¤‘ë³µëœ í–‰ì˜ ì¸ë±ìŠ¤: [1026, 5991]\n",
      "\n",
      "ì¤‘ë³µëœ í—¤ë“œë¼ì¸: ëˆˆì¹ ë¬¸ì‹  ì–´ë•Œ?\n",
      "ì¤‘ë³µëœ í–‰ì˜ ì¸ë±ìŠ¤: [1070, 6003]\n",
      "\n",
      "ì¤‘ë³µëœ í—¤ë“œë¼ì¸: ë‰´ìŠ¤ëŠ” ì—­ì‹œ ì§€ë£¨í•´\n",
      "ì¤‘ë³µëœ í–‰ì˜ ì¸ë±ìŠ¤: [1088, 6004]\n",
      "\n",
      "ì¤‘ë³µëœ í—¤ë“œë¼ì¸: ëˆ ë²Œê³  ì‹¶ì–´\n",
      "ì¤‘ë³µëœ í–‰ì˜ ì¸ë±ìŠ¤: [1293, 1294]\n",
      "\n",
      "ì¤‘ë³µëœ í—¤ë“œë¼ì¸: ë”± ì¢‹ì•„\n",
      "ì¤‘ë³µëœ í–‰ì˜ ì¸ë±ìŠ¤: [1386, 6141]\n",
      "\n",
      "ì¤‘ë³µëœ í—¤ë“œë¼ì¸: ë˜ ì „í™” ì•ˆë°›ì•„\n",
      "ì¤‘ë³µëœ í–‰ì˜ ì¸ë±ìŠ¤: [1412, 6164]\n",
      "\n",
      "ì¤‘ë³µëœ í—¤ë“œë¼ì¸: ë¡œë˜ ë²ˆí˜¸ ì•Œë ¤ì¤˜\n",
      "ì¤‘ë³µëœ í–‰ì˜ ì¸ë±ìŠ¤: [1444, 1445]\n",
      "\n",
      "ì¤‘ë³µëœ í—¤ë“œë¼ì¸: ë§ˆìŒì´ ìš¸ì í•´\n",
      "ì¤‘ë³µëœ í–‰ì˜ ì¸ë±ìŠ¤: [1480, 1481]\n",
      "\n",
      "ì¤‘ë³µëœ í—¤ë“œë¼ì¸: ë§Œë‚˜ê¸°ë§Œ í•˜ë©´ ì‹¸ì›Œ\n",
      "ì¤‘ë³µëœ í–‰ì˜ ì¸ë±ìŠ¤: [1500, 1501]\n",
      "\n",
      "ì¤‘ë³µëœ í—¤ë“œë¼ì¸: ë§ì´ ì•ˆ í†µí•´\n",
      "ì¤‘ë³µëœ í–‰ì˜ ì¸ë±ìŠ¤: [1536, 1537]\n",
      "\n",
      "ì¤‘ë³µëœ í—¤ë“œë¼ì¸: ë©´ë„ ê·€ì°¬í•˜\n",
      "ì¤‘ë³µëœ í–‰ì˜ ì¸ë±ìŠ¤: [1662, 6309]\n",
      "\n",
      "ì¤‘ë³µëœ í—¤ë“œë¼ì¸: ë©´ì ‘ ì˜ ë³¼ ìˆ˜ ìˆì„ê¹Œ\n",
      "ì¤‘ë³µëœ í–‰ì˜ ì¸ë±ìŠ¤: [1676, 6310]\n",
      "\n",
      "ì¤‘ë³µëœ í—¤ë“œë¼ì¸: ë©´ì ‘ ì¤€ë¹„ ë°©ë²•\n",
      "ì¤‘ë³µëœ í–‰ì˜ ì¸ë±ìŠ¤: [1677, 6311]\n",
      "\n",
      "ì¤‘ë³µëœ í—¤ë“œë¼ì¸: ë¬¸ ì ê²¼ëŠ”ë° ì§‘ì— ì•„ë¬´ë„ ì—†ë„¤\n",
      "ì¤‘ë³µëœ í–‰ì˜ ì¸ë±ìŠ¤: [1773, 6363]\n",
      "\n",
      "ì¤‘ë³µëœ í—¤ë“œë¼ì¸: ë­í•˜ë©´ ì‹œê°„ì´ ì˜ ê°ˆê¹Œ\n",
      "ì¤‘ë³µëœ í–‰ì˜ ì¸ë±ìŠ¤: [1834, 1835]\n",
      "\n",
      "ì¤‘ë³µëœ í—¤ë“œë¼ì¸: ë­”ê°€ ë¬´ì„­ë‹¤\n",
      "ì¤‘ë³µëœ í–‰ì˜ ì¸ë±ìŠ¤: [1845, 6378]\n",
      "\n",
      "ì¤‘ë³µëœ í—¤ë“œë¼ì¸: ë°”ë¼ëŠ”ê²Œ ì—†ì–´\n",
      "ì¤‘ë³µëœ í–‰ì˜ ì¸ë±ìŠ¤: [1897, 1898]\n",
      "\n",
      "ì¤‘ë³µëœ í—¤ë“œë¼ì¸: ë°˜ê°€ì›Œ\n",
      "ì¤‘ë³µëœ í–‰ì˜ ì¸ë±ìŠ¤: [1913, 1914]\n",
      "\n",
      "ì¤‘ë³µëœ í—¤ë“œë¼ì¸: ë°œëª© ì ‘ì§ˆë €ì–´\n",
      "ì¤‘ë³µëœ í–‰ì˜ ì¸ë±ìŠ¤: [1945, 1946]\n",
      "\n",
      "ì¤‘ë³µëœ í—¤ë“œë¼ì¸: ë°°ê³ íŒŒ\n",
      "ì¤‘ë³µëœ í–‰ì˜ ì¸ë±ìŠ¤: [2000, 2001]\n",
      "\n",
      "ì¤‘ë³µëœ í—¤ë“œë¼ì¸: ë°°ë¶ˆëŸ¬\n",
      "ì¤‘ë³µëœ í–‰ì˜ ì¸ë±ìŠ¤: [2013, 2014]\n",
      "\n",
      "ì¤‘ë³µëœ í—¤ë“œë¼ì¸: ë¹„ ì˜¤ëŠ”ë°?\n",
      "ì¤‘ë³µëœ í–‰ì˜ ì¸ë±ìŠ¤: [2197, 2198]\n",
      "\n",
      "ì¤‘ë³µëœ í—¤ë“œë¼ì¸: ë¹„ë°€ë²ˆí˜¸ ë­ì˜€ë”ë¼\n",
      "ì¤‘ë³µëœ í–‰ì˜ ì¸ë±ìŠ¤: [2208, 6552]\n",
      "\n",
      "ì¤‘ë³µëœ í—¤ë“œë¼ì¸: ë»”í•˜ë‹¤\n",
      "ì¤‘ë³µëœ í–‰ì˜ ì¸ë±ìŠ¤: [2251, 6561]\n",
      "\n",
      "ì¤‘ë³µëœ í—¤ë“œë¼ì¸: ì‚¬ë‘í•œë‹¤ê³  ë§í•´ì¤˜\n",
      "ì¤‘ë³µëœ í–‰ì˜ ì¸ë±ìŠ¤: [2283, 10001]\n",
      "\n",
      "ì¤‘ë³µëœ í—¤ë“œë¼ì¸: ì‚¬ì—… ì‹œì‘í•´ë„ ë ê¹Œ\n",
      "ì¤‘ë³µëœ í–‰ì˜ ì¸ë±ìŠ¤: [2291, 6611]\n",
      "\n",
      "ì¤‘ë³µëœ í—¤ë“œë¼ì¸: ìƒë¦¬í†µ ë•Œë¬¸ì— ë°° ì•„íŒŒ\n",
      "ì¤‘ë³µëœ í–‰ì˜ ì¸ë±ìŠ¤: [2414, 6677]\n",
      "\n",
      "ì¤‘ë³µëœ í—¤ë“œë¼ì¸: ì„ í’ê¸° í‹€ì–´ë„ ë”ì›Œ\n",
      "ì¤‘ë³µëœ í–‰ì˜ ì¸ë±ìŠ¤: [2456, 2457]\n",
      "\n",
      "ì¤‘ë³µëœ í—¤ë“œë¼ì¸: ì…€í”„ì›¨ë”©ì´ ìœ í–‰ì´ë˜\n",
      "ì¤‘ë³µëœ í–‰ì˜ ì¸ë±ìŠ¤: [2504, 6700]\n",
      "\n",
      "ì¤‘ë³µëœ í—¤ë“œë¼ì¸: ì†ì´ ì•ˆ ì¢‹ì•„\n",
      "ì¤‘ë³µëœ í–‰ì˜ ì¸ë±ìŠ¤: [2535, 2536]\n",
      "\n",
      "ì¤‘ë³µëœ í—¤ë“œë¼ì¸: ìˆ˜ì—…ì‹œê°„ ë‚´ë‚´ ì¤ì–´\n",
      "ì¤‘ë³µëœ í–‰ì˜ ì¸ë±ìŠ¤: [2584, 2585]\n",
      "\n",
      "ì¤‘ë³µëœ í—¤ë“œë¼ì¸: ìˆ  ë§ˆì‹œê³  ì‹¶ë‹¤\n",
      "ì¤‘ë³µëœ í–‰ì˜ ì¸ë±ìŠ¤: [2616, 2617]\n",
      "\n",
      "ì¤‘ë³µëœ í—¤ë“œë¼ì¸: ìˆ  ë¨¹ê³  ì‹¶ì–´\n",
      "ì¤‘ë³µëœ í–‰ì˜ ì¸ë±ìŠ¤: [2620, 6718]\n",
      "\n",
      "ì¤‘ë³µëœ í—¤ë“œë¼ì¸: ìˆ ë§Œ ë¨¹ìœ¼ë©´ ì „í™”ë¥¼ ì•ˆ ë°›ì•„\n",
      "ì¤‘ë³µëœ í–‰ì˜ ì¸ë±ìŠ¤: [2636, 6727]\n",
      "\n",
      "ì¤‘ë³µëœ í—¤ë“œë¼ì¸: ì‰¬ëŠ” ì¤‘ì…ë‹ˆë‹¤.\n",
      "ì¤‘ë³µëœ í–‰ì˜ ì¸ë±ìŠ¤: [2644, 6747]\n",
      "\n",
      "ì¤‘ë³µëœ í—¤ë“œë¼ì¸: ìŠ¤íƒ€íŠ¸ì—…í•˜ë©´ ìœ„í—˜í• ê¹Œ\n",
      "ì¤‘ë³µëœ í–‰ì˜ ì¸ë±ìŠ¤: [2675, 6751]\n",
      "\n",
      "ì¤‘ë³µëœ í—¤ë“œë¼ì¸: ìŠ¤íŠ¸ë ˆìŠ¤ ë°›ì•„\n",
      "ì¤‘ë³µëœ í–‰ì˜ ì¸ë±ìŠ¤: [2685, 2686]\n",
      "\n",
      "ì¤‘ë³µëœ í—¤ë“œë¼ì¸: ì‹ í˜¼ì—¬í–‰ ê°€ì„œ ëŒì•„ì˜¤ê¸° ì‹«ë‹¤\n",
      "ì¤‘ë³µëœ í–‰ì˜ ì¸ë±ìŠ¤: [2777, 6787]\n",
      "\n",
      "ì¤‘ë³µëœ í—¤ë“œë¼ì¸: ì‹¬ì‹¬í•´\n",
      "ì¤‘ë³µëœ í–‰ì˜ ì¸ë±ìŠ¤: [2801, 6796]\n",
      "\n",
      "ì¤‘ë³µëœ í—¤ë“œë¼ì¸: ì¸íƒ€ëŠ”ê±° ì¹œêµ¬í•œí…Œ ì´ì•¼ê¸° í•œê³  ì‹¶ë‹¤.\n",
      "ì¤‘ë³µëœ í–‰ì˜ ì¸ë±ìŠ¤: [2820, 6802]\n",
      "\n",
      "ì¤‘ë³µëœ í—¤ë“œë¼ì¸: ì•„ë¬´ë„ ì•ˆ ë†€ì•„ì¤˜\n",
      "ì¤‘ë³µëœ í–‰ì˜ ì¸ë±ìŠ¤: [2859, 6839]\n",
      "\n",
      "ì¤‘ë³µëœ í—¤ë“œë¼ì¸: ì•„ì´ë”” ìƒê° ì•ˆë‚˜\n",
      "ì¤‘ë³µëœ í–‰ì˜ ì¸ë±ìŠ¤: [2893, 6849]\n",
      "\n",
      "ì¤‘ë³µëœ í—¤ë“œë¼ì¸: ì•„íŒŒ\n",
      "ì¤‘ë³µëœ í–‰ì˜ ì¸ë±ìŠ¤: [2922, 6882]\n",
      "\n",
      "ì¤‘ë³µëœ í—¤ë“œë¼ì¸: ì• ë¥¼ ë‚˜í˜¼ì í‚¤ìš°ëŠ” ê²ƒ ê°™ì•„\n",
      "ì¤‘ë³µëœ í–‰ì˜ ì¸ë±ìŠ¤: [2988, 6907]\n",
      "\n",
      "ì¤‘ë³µëœ í—¤ë“œë¼ì¸: ì•¼\n",
      "ì¤‘ë³µëœ í–‰ì˜ ì¸ë±ìŠ¤: [2995, 6909]\n",
      "\n",
      "ì¤‘ë³µëœ í—¤ë“œë¼ì¸: ì–˜ëŠ” ì‹¸ìš°ê¸°ë§Œ í•˜ë©´ ì—°ë½ì´ ì•ˆë¼\n",
      "ì¤‘ë³µëœ í–‰ì˜ ì¸ë±ìŠ¤: [3022, 6918]\n",
      "\n",
      "ì¤‘ë³µëœ í—¤ë“œë¼ì¸: ì–¸ì œì¯¤ ì˜ˆì˜ê²Œ í™”ì¥ ì˜í• ê¹Œ\n",
      "ì¤‘ë³µëœ í–‰ì˜ ì¸ë±ìŠ¤: [3065, 7013]\n",
      "\n",
      "ì¤‘ë³µëœ í—¤ë“œë¼ì¸: ì—„ë§ˆì•„ë¹ ë‘ ë‹¤ì‹œ ê°™ì´ ì‚´ì•„ì•¼ë¼\n",
      "ì¤‘ë³µëœ í–‰ì˜ ì¸ë±ìŠ¤: [3111, 7030]\n",
      "\n",
      "ì¤‘ë³µëœ í—¤ë“œë¼ì¸: ì—…ë¬´ ìŠ¤íŠ¸ë ˆìŠ¤ ë„˜ ì‹¬í•´\n",
      "ì¤‘ë³µëœ í–‰ì˜ ì¸ë±ìŠ¤: [3121, 7031]\n",
      "\n",
      "ì¤‘ë³µëœ í—¤ë“œë¼ì¸: ì˜¤ì‹¹í•œ ì´ì•¼ê¸° í•´ì¤„ê¹Œ\n",
      "ì¤‘ë³µëœ í–‰ì˜ ì¸ë±ìŠ¤: [3375, 7210]\n",
      "\n",
      "ì¤‘ë³µëœ í—¤ë“œë¼ì¸: ì˜¬í•´ ì™œ ì´ëŸ¬ì§€\n",
      "ì¤‘ë³µëœ í–‰ì˜ ì¸ë±ìŠ¤: [3387, 3388, 3389]\n",
      "\n",
      "ì¤‘ë³µëœ í—¤ë“œë¼ì¸: ì™¸ë¡œì›Œ\n",
      "ì¤‘ë³µëœ í–‰ì˜ ì¸ë±ìŠ¤: [3456, 3457, 7261, 7262]\n",
      "\n",
      "ì¤‘ë³µëœ í—¤ë“œë¼ì¸: ìš°ì •ì´ë€ê²Œ ë­˜ê¹Œ\n",
      "ì¤‘ë³µëœ í–‰ì˜ ì¸ë±ìŠ¤: [3532, 7292]\n",
      "\n",
      "ì¤‘ë³µëœ í—¤ë“œë¼ì¸: ì›ƒê²¨ë´\n",
      "ì¤‘ë³µëœ í–‰ì˜ ì¸ë±ìŠ¤: [3564, 7299]\n",
      "\n",
      "ì¤‘ë³µëœ í—¤ë“œë¼ì¸: ì´ ìˆœê°„ ë­˜í•˜ë©´ ì¢‹ì„ê¹Œ\n",
      "ì¤‘ë³µëœ í–‰ì˜ ì¸ë±ìŠ¤: [3654, 7343]\n",
      "\n",
      "ì¤‘ë³µëœ í—¤ë“œë¼ì¸: ì¼ì´ ì•ˆ ëë‚˜\n",
      "ì¤‘ë³µëœ í–‰ì˜ ì¸ë±ìŠ¤: [3834, 7786]\n",
      "\n",
      "ì¤‘ë³µëœ í—¤ë“œë¼ì¸: ì¼ì´ ìµìˆ™í•´ ì§€ì§€ ì•Šë„¤\n",
      "ì¤‘ë³µëœ í–‰ì˜ ì¸ë±ìŠ¤: [3835, 7787]\n",
      "\n",
      "ì¤‘ë³µëœ í—¤ë“œë¼ì¸: ì„ìš© ê¸°ë‹¤ë¦¬ê³  ìˆì–´\n",
      "ì¤‘ë³µëœ í–‰ì˜ ì¸ë±ìŠ¤: [3858, 3859]\n",
      "\n",
      "ì¤‘ë³µëœ í—¤ë“œë¼ì¸: ìê¾¸ ë´ì£¼ë‹ˆê¹Œ ê¸°ì–´ì˜¤ë¥¸ë‹¤\n",
      "ì¤‘ë³µëœ í–‰ì˜ ì¸ë±ìŠ¤: [3909, 7843]\n",
      "\n",
      "ì¤‘ë³µëœ í—¤ë“œë¼ì¸: ìê¾¸ ì¡¸ê²Œ ë˜ë„¤\n",
      "ì¤‘ë³µëœ í–‰ì˜ ì¸ë±ìŠ¤: [3916, 7849]\n",
      "\n",
      "ì¤‘ë³µëœ í—¤ë“œë¼ì¸: ìë‹ˆ?\n",
      "ì¤‘ë³µëœ í–‰ì˜ ì¸ë±ìŠ¤: [3921, 3922]\n",
      "\n",
      "ì¤‘ë³µëœ í—¤ë“œë¼ì¸: ìì¡´ê°ì´ ë„ˆë¬´ ë–¨ì–´ì¡Œì–´\n",
      "ì¤‘ë³µëœ í–‰ì˜ ì¸ë±ìŠ¤: [3945, 3946]\n",
      "\n",
      "ì¤‘ë³µëœ í—¤ë“œë¼ì¸: ì € ì‚¬ëŒì´ ì™œ ìê¾¸ ë³¼ê¹Œ?\n",
      "ì¤‘ë³µëœ í–‰ì˜ ì¸ë±ìŠ¤: [4028, 7941]\n",
      "\n",
      "ì¤‘ë³µëœ í—¤ë“œë¼ì¸: ì£¼ì°¨ì¥ì´ ê½‰ ì°¼ì–´\n",
      "ì¤‘ë³µëœ í–‰ì˜ ì¸ë±ìŠ¤: [4235, 8159]\n",
      "\n",
      "ì¤‘ë³µëœ í—¤ë“œë¼ì¸: ì¤‘2 íˆìŠ¤í…Œë¦¬ ì•Œì•„?\n",
      "ì¤‘ë³µëœ í–‰ì˜ ì¸ë±ìŠ¤: [4249, 8176]\n",
      "\n",
      "ì¤‘ë³µëœ í—¤ë“œë¼ì¸: ì¤‘êµ­ì–´ í˜¼ì ê³µë¶€ ê°€ëŠ¥í•œê°€?\n",
      "ì¤‘ë³µëœ í–‰ì˜ ì¸ë±ìŠ¤: [4264, 8177]\n",
      "\n",
      "ì¤‘ë³µëœ í—¤ë“œë¼ì¸: ìµœì €ì„ê¸ˆ ìˆ˜ì¤€ì—ì„œ ì•Œë°”ë¹„ê°€ ì•ˆ ë‚˜ì™”ì–´\n",
      "ì¤‘ë³µëœ í–‰ì˜ ì¸ë±ìŠ¤: [4505, 8328]\n",
      "\n",
      "ì¤‘ë³µëœ í—¤ë“œë¼ì¸: ì¶œê·¼ì‹œê°„ ì•„ê¹ë‹¤\n",
      "ì¤‘ë³µëœ í–‰ì˜ ì¸ë±ìŠ¤: [4533, 8338]\n",
      "\n",
      "ì¤‘ë³µëœ í—¤ë“œë¼ì¸: ì¹´í†¡í•  ì¹œêµ¬ê°€ ì—†ë„¤\n",
      "ì¤‘ë³µëœ í–‰ì˜ ì¸ë±ìŠ¤: [4704, 4705]\n",
      "\n",
      "ì¤‘ë³µëœ í—¤ë“œë¼ì¸: ì»´í„°ê°€ ë§›ì´ ê°”ì–´\n",
      "ì¤‘ë³µëœ í–‰ì˜ ì¸ë±ìŠ¤: [4744, 8359]\n",
      "\n",
      "ì¤‘ë³µëœ í—¤ë“œë¼ì¸: ì»´í“¨í„°ê°€ ëŠë ¤ì¡Œì–´\n",
      "ì¤‘ë³µëœ í–‰ì˜ ì¸ë±ìŠ¤: [4748, 4749]\n",
      "\n",
      "ì¤‘ë³µëœ í—¤ë“œë¼ì¸: íƒì‹œë¹„ ë„ˆë¬´ ë¹„ì‹¸\n",
      "ì¤‘ë³µëœ í–‰ì˜ ì¸ë±ìŠ¤: [4810, 8364]\n",
      "\n",
      "ì¤‘ë³µëœ í—¤ë“œë¼ì¸: í‰ìƒ í•¨ê»˜í•˜ê³  ì‹¶ë‹¤.\n",
      "ì¤‘ë³µëœ í–‰ì˜ ì¸ë±ìŠ¤: [4880, 8381]\n",
      "\n",
      "ì¤‘ë³µëœ í—¤ë“œë¼ì¸: í•˜ê³  ì‹¶ì€ ê²Œ ì—†ì–´\n",
      "ì¤‘ë³µëœ í–‰ì˜ ì¸ë±ìŠ¤: [4946, 4947]\n",
      "\n",
      "ì¤‘ë³µëœ í—¤ë“œë¼ì¸: í•˜ê³  ì‹¶ì€ê²Œ ì—†ì–´\n",
      "ì¤‘ë³µëœ í–‰ì˜ ì¸ë±ìŠ¤: [4949, 8397]\n",
      "\n",
      "ì¤‘ë³µëœ í—¤ë“œë¼ì¸: í•  ì¤„ ì•„ëŠ”ê±°!\n",
      "ì¤‘ë³µëœ í–‰ì˜ ì¸ë±ìŠ¤: [5030, 8470]\n",
      "\n",
      "ì¤‘ë³µëœ í—¤ë“œë¼ì¸: í•¸ë“œí° êº¼ì§€ê¸° ì§ì „\n",
      "ì¤‘ë³µëœ í–‰ì˜ ì¸ë±ìŠ¤: [5058, 5059]\n",
      "\n",
      "ì¤‘ë³µëœ í—¤ë“œë¼ì¸: í•¸ë“œí° ë–¨ì–´ëœ¨ë ¤ì„œ ê³ ì¥ ë‚¬ë‚˜ë´\n",
      "ì¤‘ë³µëœ í–‰ì˜ ì¸ë±ìŠ¤: [5063, 8476]\n",
      "\n",
      "ì¤‘ë³µëœ í—¤ë“œë¼ì¸: í˜¼ì ì‚´ì•„ì•¼ í•  ë“¯\n",
      "ì¤‘ë³µëœ í–‰ì˜ ì¸ë±ìŠ¤: [5146, 8741]\n",
      "\n",
      "ì¤‘ë³µëœ í—¤ë“œë¼ì¸: í˜¼ì ìˆìœ¼ë‹ˆê¹Œ í¸í•˜ë„¤\n",
      "ì¤‘ë³µëœ í–‰ì˜ ì¸ë±ìŠ¤: [5154, 8748]\n",
      "\n",
      "ì¤‘ë³µëœ í—¤ë“œë¼ì¸: í˜¼ì ì˜ ì‚´ ìˆ˜ ìˆì„ê¹Œ?\n",
      "ì¤‘ë³µëœ í–‰ì˜ ì¸ë±ìŠ¤: [5157, 8749]\n",
      "\n",
      "ì¤‘ë³µëœ í—¤ë“œë¼ì¸: í˜¼ì í•´ì•¼ ë¼\n",
      "ì¤‘ë³µëœ í–‰ì˜ ì¸ë±ìŠ¤: [5159, 8751]\n",
      "\n",
      "ì¤‘ë³µëœ í—¤ë“œë¼ì¸: í™”ì¥ì‹¤!!\n",
      "ì¤‘ë³µëœ í–‰ì˜ ì¸ë±ìŠ¤: [5191, 8761]\n",
      "\n",
      "ì¤‘ë³µëœ í—¤ë“œë¼ì¸: í™˜ìŠ¹ ê°€ëŠ¥?\n",
      "ì¤‘ë³µëœ í–‰ì˜ ì¸ë±ìŠ¤: [5207, 8764]\n",
      "\n",
      "ì¤‘ë³µëœ í—¤ë“œë¼ì¸: íšŒì‚¬ ì‚¬ëŒë“¤ì´ ì•„ì§ë„ ë¶ˆí¸í•´\n",
      "ì¤‘ë³µëœ í–‰ì˜ ì¸ë±ìŠ¤: [5218, 8780]\n",
      "\n",
      "ì¤‘ë³µëœ í—¤ë“œë¼ì¸: íšŒì‚¬ì—ëŠ” ì™œ ì¹œêµ¬ ê°™ì€ ì‚¬ëŒì´ ì—†ì„ê¹Œ\n",
      "ì¤‘ë³µëœ í–‰ì˜ ì¸ë±ìŠ¤: [5232, 8782]\n",
      "\n",
      "ì¤‘ë³µëœ í—¤ë“œë¼ì¸: í›„ë ¨í•˜ë‹¬ê¹Œ\n",
      "ì¤‘ë³µëœ í–‰ì˜ ì¸ë±ìŠ¤: [5246, 8789]\n",
      "\n",
      "ì¤‘ë³µëœ í—¤ë“œë¼ì¸: 2ë…„ ê°€ëŸ‰ì˜ ì—°ì• \n",
      "ì¤‘ë³µëœ í–‰ì˜ ì¸ë±ìŠ¤: [5316, 8868]\n",
      "\n",
      "ì¤‘ë³µëœ í—¤ë“œë¼ì¸: ê°„ë‹¨í•˜ê²Œ ì‚¬ë‘ì´ë¼ëŠ”ê±´.\n",
      "ì¤‘ë³µëœ í–‰ì˜ ì¸ë±ìŠ¤: [5438, 5439]\n",
      "\n",
      "ì¤‘ë³µëœ í—¤ë“œë¼ì¸: ê²°êµ­ í•¸ë“œí° ë²ˆí˜¸ ë°”ê¿¨ì–´\n",
      "ì¤‘ë³µëœ í–‰ì˜ ì¸ë±ìŠ¤: [5508, 5509]\n",
      "\n",
      "ì¤‘ë³µëœ í—¤ë“œë¼ì¸: ê²°êµ­ í—¤ì–´ì¡Œë„¤\n",
      "ì¤‘ë³µëœ í–‰ì˜ ì¸ë±ìŠ¤: [5510, 5511]\n",
      "\n",
      "ì¤‘ë³µëœ í—¤ë“œë¼ì¸: ë‚˜ë„ ëª¨ë¥´ê²Œ ë‹ˆ ìƒê°ì„ í•˜ê³  ìˆì–´\n",
      "ì¤‘ë³µëœ í–‰ì˜ ì¸ë±ìŠ¤: [5776, 9171]\n",
      "\n",
      "ì¤‘ë³µëœ í—¤ë“œë¼ì¸: ë„ˆë¬´ í˜ë“­ë‹ˆë‹¤\n",
      "ì¤‘ë³µëœ í–‰ì˜ ì¸ë±ìŠ¤: [5953, 5954]\n",
      "\n",
      "ì¤‘ë³µëœ í—¤ë“œë¼ì¸: ë¯¸ì¹˜ê² ë„¤\n",
      "ì¤‘ë³µëœ í–‰ì˜ ì¸ë±ìŠ¤: [6398, 6399]\n",
      "\n",
      "ì¤‘ë³µëœ í—¤ë“œë¼ì¸: ë¯¸ì¹˜ê² ìŠµë‹ˆë‹¤\n",
      "ì¤‘ë³µëœ í–‰ì˜ ì¸ë±ìŠ¤: [6400, 6401]\n",
      "\n",
      "ì¤‘ë³µëœ í—¤ë“œë¼ì¸: ì‚¬ë‘ì´ ë­˜ê¹Œ?\n",
      "ì¤‘ë³µëœ í–‰ì˜ ì¸ë±ìŠ¤: [6600, 9923]\n",
      "\n",
      "ì¤‘ë³µëœ í—¤ë“œë¼ì¸: ìƒê°í•  ì‹œê°„ì„ ë‹¬ë¼ê³  í•œ ë‚¨ì¹œ\n",
      "ì¤‘ë³µëœ í–‰ì˜ ì¸ë±ìŠ¤: [6674, 6675]\n",
      "\n",
      "ì¤‘ë³µëœ í—¤ë“œë¼ì¸: ì—°ë½í•˜ê³  ì‹¶ì–´\n",
      "ì¤‘ë³µëœ í–‰ì˜ ì¸ë±ìŠ¤: [7095, 10709]\n",
      "\n",
      "ì¤‘ë³µëœ í—¤ë“œë¼ì¸: ì´ë³„ 6ì¼ì§¸\n",
      "ì¤‘ë³µëœ í–‰ì˜ ì¸ë±ìŠ¤: [7413, 7414]\n",
      "\n",
      "ì¤‘ë³µëœ í—¤ë“œë¼ì¸: ì´ë³„ì´ë€\n",
      "ì¤‘ë³µëœ í–‰ì˜ ì¸ë±ìŠ¤: [7561, 7562]\n",
      "\n",
      "ì¤‘ë³µëœ í—¤ë“œë¼ì¸: ì •ë§ í˜ë“œë„¤\n",
      "ì¤‘ë³µëœ í–‰ì˜ ì¸ë±ìŠ¤: [8067, 8068]\n",
      "\n",
      "ì¤‘ë³µëœ í—¤ë“œë¼ì¸: í—¤ì–´ì§„ì§€ í•œë‹¬\n",
      "ì¤‘ë³µëœ í–‰ì˜ ì¸ë±ìŠ¤: [8690, 8710]\n",
      "\n",
      "ì¤‘ë³µëœ í—¤ë“œë¼ì¸: ê°€ì„ íƒ€ë‚˜ ë´.\n",
      "ì¤‘ë³µëœ í–‰ì˜ ì¸ë±ìŠ¤: [8901, 8902]\n",
      "\n",
      "ì¤‘ë³µëœ í—¤ë“œë¼ì¸: ê³¼í•¨ ì„¤ë ˜ í›„ì— ì§€ê¸ˆì€ ì•ˆ ì„¤ë ˆ\n",
      "ì¤‘ë³µëœ í–‰ì˜ ì¸ë±ìŠ¤: [9017, 9018]\n",
      "\n",
      "ì¤‘ë³µëœ í—¤ë“œë¼ì¸: ë‚¨ìì¹œêµ¬ë‘ ë§ì´ ì•ˆ í†µí•´\n",
      "ì¤‘ë³µëœ í–‰ì˜ ì¸ë±ìŠ¤: [9372, 9373]\n",
      "\n",
      "ì¤‘ë³µëœ í—¤ë“œë¼ì¸: ë‚¨í¸ì´ ìê¾¸ ë§Œì ¸\n",
      "ì¤‘ë³µëœ í–‰ì˜ ì¸ë±ìŠ¤: [9441, 9442]\n",
      "\n",
      "ì¤‘ë³µëœ í—¤ë“œë¼ì¸: ë‚´ê°€ ì¢‹ì•„í•  ìê²©ì´ ìˆë‚˜ ëª¨ë¥´ê² ì–´.\n",
      "ì¤‘ë³µëœ í–‰ì˜ ì¸ë±ìŠ¤: [9526, 9527]\n",
      "\n",
      "ì¤‘ë³µëœ í—¤ë“œë¼ì¸: ë‚´ê°€ ì¢‹ì•„í•´ë„ ë ê¹Œ?\n",
      "ì¤‘ë³µëœ í–‰ì˜ ì¸ë±ìŠ¤: [9528, 9529]\n",
      "\n",
      "ì¤‘ë³µëœ í—¤ë“œë¼ì¸: ë„ˆë¬´ í–‰ë³µí•´. ì˜¤ë˜ê°ˆ ìˆ˜ ìˆì„ê¹Œ?\n",
      "ì¤‘ë³µëœ í–‰ì˜ ì¸ë±ìŠ¤: [9559, 9560]\n",
      "\n",
      "ì¤‘ë³µëœ í—¤ë“œë¼ì¸: ë§ˆìŒì— ì—†ëŠ”ë° ê´€ê³„ë¥¼ í•˜ê³  ì‹¶ëŒ€\n",
      "ì¤‘ë³µëœ í–‰ì˜ ì¸ë±ìŠ¤: [9674, 9675]\n",
      "\n",
      "ì¤‘ë³µëœ í—¤ë“œë¼ì¸: ë´„ íƒ€ë‚˜ ë´„.\n",
      "ì¤‘ë³µëœ í–‰ì˜ ì¸ë±ìŠ¤: [9801, 9802]\n",
      "\n",
      "ì¤‘ë³µëœ í—¤ë“œë¼ì¸: ë¶ˆíƒ€ì˜¬ëë‹¤ê°€ ì‹ì—ˆì–´\n",
      "ì¤‘ë³µëœ í–‰ì˜ ì¸ë±ìŠ¤: [9812, 9813]\n",
      "\n",
      "ì¤‘ë³µëœ í—¤ë“œë¼ì¸: ì‚¬ê·€ìëŠ” ë§ ì•„ë‹ˆë©´ ì¸ì¸ê°€\n",
      "ì¤‘ë³µëœ í–‰ì˜ ì¸ë±ìŠ¤: [9833, 9834]\n",
      "\n",
      "ì¤‘ë³µëœ í—¤ë“œë¼ì¸: ì‚¬ë‘ì— ë¹ ì§„ ê±° ê°™ì•„\n",
      "ì¤‘ë³µëœ í–‰ì˜ ì¸ë±ìŠ¤: [9881, 9882]\n",
      "\n",
      "ì¤‘ë³µëœ í—¤ë“œë¼ì¸: ì‚¬ë‘ì„ í–ˆë‹¤\n",
      "ì¤‘ë³µëœ í–‰ì˜ ì¸ë±ìŠ¤: [9897, 9898, 9899]\n",
      "\n",
      "ì¤‘ë³µëœ í—¤ë“œë¼ì¸: ì‚¬ë‘ì´ ë­ì•¼?\n",
      "ì¤‘ë³µëœ í–‰ì˜ ì¸ë±ìŠ¤: [9917, 9918]\n",
      "\n",
      "ì¤‘ë³µëœ í—¤ë“œë¼ì¸: ì‚¬ë‘í•˜ëŠ” ì‚¬ëŒì´ë‘ ê²°í˜¼í•˜ê³ ì‹¶ì–´\n",
      "ì¤‘ë³µëœ í–‰ì˜ ì¸ë±ìŠ¤: [9962, 9963]\n",
      "\n",
      "ì¤‘ë³µëœ í—¤ë“œë¼ì¸: ì‚¬ë‘í•˜ë©´ ì˜ˆë»ì§€ë‚˜ë´\n",
      "ì¤‘ë³µëœ í–‰ì˜ ì¸ë±ìŠ¤: [9991, 9992]\n",
      "\n",
      "ì¤‘ë³µëœ í—¤ë“œë¼ì¸: ì‚¬ë‘í•´\n",
      "ì¤‘ë³µëœ í–‰ì˜ ì¸ë±ìŠ¤: [10014, 10015]\n",
      "\n",
      "ì¤‘ë³µëœ í—¤ë“œë¼ì¸: ì¸ íƒ€ëŠ” ê¸°ê°„\n",
      "ì¤‘ë³µëœ í–‰ì˜ ì¸ë±ìŠ¤: [10183, 10184]\n",
      "\n",
      "ì¤‘ë³µëœ í—¤ë“œë¼ì¸: ì¸ì´ì—ˆìœ¼ë©´ ì¢‹ê² ì–´\n",
      "ì¤‘ë³µëœ í–‰ì˜ ì¸ë±ìŠ¤: [10380, 10381]\n",
      "\n",
      "ì¤‘ë³µëœ í—¤ë“œë¼ì¸: ì”¨ì”¨ì¸ë° ëª°ë˜ ë§Œë‚˜ê³  ìˆëŠ”ë° ê³µê°œ ì—°ì•  í•˜ìê³  í• ê¹Œ?\n",
      "ì¤‘ë³µëœ í–‰ì˜ ì¸ë±ìŠ¤: [10432, 10433]\n",
      "\n",
      "ì¤‘ë³µëœ í—¤ë“œë¼ì¸: ì•„ê¸° ì¢‹ì•„í•˜ëŠ” ë‚¨ì ì–´ë•Œ?\n",
      "ì¤‘ë³µëœ í–‰ì˜ ì¸ë±ìŠ¤: [10436, 10437]\n",
      "\n",
      "ì¤‘ë³µëœ í—¤ë“œë¼ì¸: ì•„ê¸° ì¢‹ì•„í•˜ëŠ” ì—¬ì ì–´ë•Œ?\n",
      "ì¤‘ë³µëœ í–‰ì˜ ì¸ë±ìŠ¤: [10438, 10439]\n",
      "\n",
      "ì¤‘ë³µëœ í—¤ë“œë¼ì¸: ì—¬ìì¹œêµ¬ê°€ ì—°ë½ì´ ì•ˆë¼\n",
      "ì¤‘ë³µëœ í–‰ì˜ ì¸ë±ìŠ¤: [10612, 10613]\n",
      "\n",
      "ì¤‘ë³µëœ í—¤ë“œë¼ì¸: ì—¬ìì¹œêµ¬í•œí…Œ ë” ì´ìƒ ì„¤ë ˆì§€ ì•Šì•„\n",
      "ì¤‘ë³µëœ í–‰ì˜ ì¸ë±ìŠ¤: [10670, 10671]\n",
      "\n",
      "ì¤‘ë³µëœ í—¤ë“œë¼ì¸: ì˜í™” ë³´ìëŠ”ë° ë‚˜í•œí…Œ ê´€ì‹¬ ìˆë‚˜?\n",
      "ì¤‘ë³µëœ í–‰ì˜ ì¸ë±ìŠ¤: [10764, 10765]\n",
      "\n",
      "ì¤‘ë³µëœ í—¤ë“œë¼ì¸: ì´ ì‚¬ëŒ ë§Œë‚˜ë©´ì„œ ë‚´ ìì¡´ê°ì´ ë–¨ì–´ì§€ëŠ” ê²ƒ ê°™ì•„ ì•ˆ ë§Œë‚˜ëŠ”ê²Œ ë‹µì´ì•¼?\n",
      "ì¤‘ë³µëœ í–‰ì˜ ì¸ë±ìŠ¤: [10888, 10889]\n",
      "\n",
      "ì¤‘ë³µëœ í—¤ë“œë¼ì¸: ì¸ì—°ì´ ìˆë‹¤ê³  ìƒê°í•´?\n",
      "ì¤‘ë³µëœ í–‰ì˜ ì¸ë±ìŠ¤: [10939, 10940]\n",
      "\n",
      "ì¤‘ë³µëœ í—¤ë“œë¼ì¸: ì¥ë‚œì¸ì§€ ì§„ì‹¬ì¸ì§€ êµ¬ë¶„ì´ ì•ˆë¼.\n",
      "ì¤‘ë³µëœ í–‰ì˜ ì¸ë±ìŠ¤: [10981, 10982]\n",
      "\n",
      "ì¤‘ë³µëœ í—¤ë“œë¼ì¸: ì¢‹ì•„í•˜ëŠ” ê°ì •ì´ ë‚˜ë¥¼ ìŠ¬í”„ê²Œë„ í•´\n",
      "ì¤‘ë³µëœ í–‰ì˜ ì¸ë±ìŠ¤: [11029, 11030]\n",
      "\n",
      "ì¤‘ë³µëœ í—¤ë“œë¼ì¸: ì¢‹ì•„í•˜ëŠ” ê±°ë‘ ì‚¬ë‘í•˜ëŠ” ê±´ ì–´ë–»ê²Œ êµ¬ë¶„í•´?\n",
      "ì¤‘ë³µëœ í–‰ì˜ ì¸ë±ìŠ¤: [11034, 11035]\n",
      "\n",
      "ì¤‘ë³µëœ í—¤ë“œë¼ì¸: ì¢‹ì•„í•˜ëŠ” ê±°ë‘ ì‚¬ë‘í•˜ëŠ”ê²Œ ë‹¤ë¥¸ ê±°ì•¼?\n",
      "ì¤‘ë³µëœ í–‰ì˜ ì¸ë±ìŠ¤: [11036, 11037]\n",
      "\n",
      "ì¤‘ë³µëœ í—¤ë“œë¼ì¸: ì¢‹ì•„í•˜ëŠ”ì§€ í™•ì‹ ì´ ì•ˆë“¤ì–´\n",
      "ì¤‘ë³µëœ í–‰ì˜ ì¸ë±ìŠ¤: [11264, 11265]\n",
      "\n",
      "ì¤‘ë³µëœ í—¤ë“œë¼ì¸: ì°©í•´ì„œ ì˜í•´ì£¼ëŠ” ê±´ì§€ ì¢‹ì•„í•˜ëŠ” ê±´ì§€\n",
      "ì¤‘ë³µëœ í–‰ì˜ ì¸ë±ìŠ¤: [11641, 11642]\n",
      "\n",
      "ì¤‘ë³µëœ í—¤ë“œë¼ì¸: ì²« ëˆˆì— ë°˜í•˜ëŠ”ê²Œ ê°€ëŠ¥í•´?\n",
      "ì¤‘ë³µëœ í–‰ì˜ ì¸ë±ìŠ¤: [11648, 11649]\n",
      "\n",
      "ì¤‘ë³µëœ í—¤ë“œë¼ì¸: ì²«ì‚¬ë‘ ìƒê°ë‚˜\n",
      "ì¤‘ë³µëœ í–‰ì˜ ì¸ë±ìŠ¤: [11657, 11658]\n",
      "\n",
      "ì¤‘ë³µëœ í—¤ë“œë¼ì¸: ì»¤í”Œì—¬í–‰ì´ ë‚˜ì„ê¹Œ ê·¸ëƒ¥ ìš°ë¦¬ë¼ë¦¬ ê°ˆê¹Œ?\n",
      "ì¤‘ë³µëœ í–‰ì˜ ì¸ë±ìŠ¤: [11731, 11732]\n",
      "\n",
      "ì¤‘ë³µëœ í—¤ë“œë¼ì¸: í›”ì³ë³´ëŠ” ê²ƒë„ ëˆˆì¹˜ ë³´ì„.\n",
      "ì¤‘ë³µëœ í–‰ì˜ ì¸ë±ìŠ¤: [11818, 11819]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ì¤‘ë³µì¸ ì—´ì˜ ë°ì´í„°ë¥¼ í™•ì¸í•˜ê¸°\n",
    "duplicate_indices = []\n",
    "\n",
    "unique_Q = data['Q'].unique()\n",
    "\n",
    "for unique in unique_Q:\n",
    "    indices = data[data['Q'] == unique].index.tolist()\n",
    "    if len(indices) > 1:\n",
    "        print(f\"ì¤‘ë³µëœ í—¤ë“œë¼ì¸: {unique}\")\n",
    "        print(f\"ì¤‘ë³µëœ í–‰ì˜ ì¸ë±ìŠ¤: {indices}\")\n",
    "        duplicate_indices.extend(indices)\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Q</th>\n",
       "      <th>A</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>152</th>\n",
       "      <td>ê²°í˜¼ì´ë‚˜ í•˜ì§€ ì™œ ìê¾¸ ë‚˜í•œí…Œ í™” ë‚´ëƒêµ¬!</td>\n",
       "      <td>í˜ë“¤ê² ë„¤ìš”.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5527</th>\n",
       "      <td>ê²°í˜¼ì´ë‚˜ í•˜ì§€ ì™œ ìê¾¸ ë‚˜í•œí…Œ í™” ë‚´ëƒêµ¬!</td>\n",
       "      <td>í˜ë“¤ê² ë„¤ìš”.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>189</th>\n",
       "      <td>ê³ ë°±í•˜ê³  í›„íšŒí•˜ë©´ ì–´ë–¡í•˜ì§€</td>\n",
       "      <td>í›„íšŒëŠ” í›„íšŒë¥¼ ë‚³ì„ë¿ì´ì—ìš”. ìš©ê¸° ë‚´ì„¸ìš”.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5537</th>\n",
       "      <td>ê³ ë°±í•˜ê³  í›„íšŒí•˜ë©´ ì–´ë–¡í•˜ì§€</td>\n",
       "      <td>í›„íšŒëŠ” í›„íšŒë¥¼ ë‚³ì„ë¿ì´ì—ìš”. ìš©ê¸° ë‚´ì„¸ìš”.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>195</th>\n",
       "      <td>ê³ ì–‘ì´ í‚¤ìš°ê³  ì‹¶ì–´</td>\n",
       "      <td>ìì‹ ì„ ë¨¼ì € í‚¤ìš°ì„¸ìš”.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11658</th>\n",
       "      <td>ì²«ì‚¬ë‘ ìƒê°ë‚˜</td>\n",
       "      <td>ì§€ê¸ˆì˜ ì‚¬ë‘ì— ì¶©ì‹¤í•˜ì„¸ìš”.</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11731</th>\n",
       "      <td>ì»¤í”Œì—¬í–‰ì´ ë‚˜ì„ê¹Œ ê·¸ëƒ¥ ìš°ë¦¬ë¼ë¦¬ ê°ˆê¹Œ?</td>\n",
       "      <td>ì €ëŠ” ë‘˜ì´ ê°€ëŠ” ê±¸ ì¢‹ì•„í•´ìš”.</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11732</th>\n",
       "      <td>ì»¤í”Œì—¬í–‰ì´ ë‚˜ì„ê¹Œ ê·¸ëƒ¥ ìš°ë¦¬ë¼ë¦¬ ê°ˆê¹Œ?</td>\n",
       "      <td>ì €ëŠ” ë‘˜ì´ ê°€ëŠ” ê²Œ ì¢‹ì•„ìš”.</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11818</th>\n",
       "      <td>í›”ì³ë³´ëŠ” ê²ƒë„ ëˆˆì¹˜ ë³´ì„.</td>\n",
       "      <td>í‹°ê°€ ë‚˜ë‹ˆê¹Œ ëˆˆì¹˜ê°€ ë³´ì´ëŠ” ê±°ì£ !</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11819</th>\n",
       "      <td>í›”ì³ë³´ëŠ” ê²ƒë„ ëˆˆì¹˜ ë³´ì„.</td>\n",
       "      <td>í›”ì³ë³´ëŠ” ê±° í‹°ë‚˜ë‚˜ë´ìš”.</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>317 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                             Q                        A  label\n",
       "152    ê²°í˜¼ì´ë‚˜ í•˜ì§€ ì™œ ìê¾¸ ë‚˜í•œí…Œ í™” ë‚´ëƒêµ¬!                   í˜ë“¤ê² ë„¤ìš”.      0\n",
       "5527   ê²°í˜¼ì´ë‚˜ í•˜ì§€ ì™œ ìê¾¸ ë‚˜í•œí…Œ í™” ë‚´ëƒêµ¬!                   í˜ë“¤ê² ë„¤ìš”.      1\n",
       "189             ê³ ë°±í•˜ê³  í›„íšŒí•˜ë©´ ì–´ë–¡í•˜ì§€  í›„íšŒëŠ” í›„íšŒë¥¼ ë‚³ì„ë¿ì´ì—ìš”. ìš©ê¸° ë‚´ì„¸ìš”.      0\n",
       "5537            ê³ ë°±í•˜ê³  í›„íšŒí•˜ë©´ ì–´ë–¡í•˜ì§€  í›„íšŒëŠ” í›„íšŒë¥¼ ë‚³ì„ë¿ì´ì—ìš”. ìš©ê¸° ë‚´ì„¸ìš”.      1\n",
       "195                 ê³ ì–‘ì´ í‚¤ìš°ê³  ì‹¶ì–´             ìì‹ ì„ ë¨¼ì € í‚¤ìš°ì„¸ìš”.      0\n",
       "...                        ...                      ...    ...\n",
       "11658                  ì²«ì‚¬ë‘ ìƒê°ë‚˜           ì§€ê¸ˆì˜ ì‚¬ë‘ì— ì¶©ì‹¤í•˜ì„¸ìš”.      2\n",
       "11731    ì»¤í”Œì—¬í–‰ì´ ë‚˜ì„ê¹Œ ê·¸ëƒ¥ ìš°ë¦¬ë¼ë¦¬ ê°ˆê¹Œ?         ì €ëŠ” ë‘˜ì´ ê°€ëŠ” ê±¸ ì¢‹ì•„í•´ìš”.      2\n",
       "11732    ì»¤í”Œì—¬í–‰ì´ ë‚˜ì„ê¹Œ ê·¸ëƒ¥ ìš°ë¦¬ë¼ë¦¬ ê°ˆê¹Œ?          ì €ëŠ” ë‘˜ì´ ê°€ëŠ” ê²Œ ì¢‹ì•„ìš”.      2\n",
       "11818           í›”ì³ë³´ëŠ” ê²ƒë„ ëˆˆì¹˜ ë³´ì„.       í‹°ê°€ ë‚˜ë‹ˆê¹Œ ëˆˆì¹˜ê°€ ë³´ì´ëŠ” ê±°ì£ !      2\n",
       "11819           í›”ì³ë³´ëŠ” ê²ƒë„ ëˆˆì¹˜ ë³´ì„.            í›”ì³ë³´ëŠ” ê±° í‹°ë‚˜ë‚˜ë´ìš”.      2\n",
       "\n",
       "[317 rows x 3 columns]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "duplicate_data = data.loc[duplicate_indices]\n",
    "duplicate_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ì „ì²´ ìƒ˜í”Œìˆ˜ : 7779\n"
     ]
    }
   ],
   "source": [
    "# ì§ˆë¬¸ì´ ê²¹ì¹˜ëŠ” ê²½ìš°, í•­ìƒ ë‹µë³€ì´ ì¼ì¹˜í•˜ì§€ëŠ” ì•ŠëŠ” ê²ƒì„ í™•ì¸\n",
    "# => ë‹µë³€ì„ ê¸°ì¤€ìœ¼ë¡œ drop (ê°™ì€ ì§ˆë¬¸ë„ ë‹¤ë¥´ê²Œ ë‹µë³€í•  ìˆ˜ ìˆë„ë¡)\n",
    "data.drop_duplicates(subset = ['A'], inplace=True)\n",
    "print('ì „ì²´ ìƒ˜í”Œìˆ˜ :', (len(data)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q        0\n",
      "A        0\n",
      "label    0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(data.isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "í•œêµ­ì–´ì™€ ì˜ì–´ ì „ì²˜ë¦¬ ê³¼ì •ì˜ ì°¨ì´(https://haru0229.tistory.com/57)\n",
    "\n",
    "í•œêµ­ì–´ëŠ” ì˜ì–´ì™€ ë‹¬ë¦¬ ë„ì–´ì“°ê¸°ë¡œ êµ¬ë¶„í•˜ê¸° ì–´ë µë‹¤ (ì˜ì¡´ í˜•íƒœì†Œ ë–„ë¬¸) => ë„ì–´ì“°ê¸°ë¡œ í† í°í™”í•˜ëŠ”ê²Œ ì•„ë‹ˆë¼ í˜•íƒœì†Œ í† í°í™”ë¥¼ ìˆ˜í–‰í•´ì•¼í•¨ \n",
    "+ ì „ì²˜ë¦¬í•˜ë©´ì„œ ìƒê¸´ ë¬¸ì œ ë° í•´ê²°ë°©ì•ˆ\n",
    "1. í˜•íƒœì†Œ í† í°í™” ìˆ˜í–‰\n",
    "2. ì˜ì¡´ í˜•íƒœì†Œë¥¼ ë¶ˆìš©ì–´ ì²˜ë¦¬ (Q(ì§ˆë¬¸)ì—ë§Œ ì ìš© , A(ë‹µë³€)ì€ ìì—°ìŠ¤ëŸ½ê²Œ ì¶œë ¥ë  ìˆ˜ ìˆë„ë¡ ë¶ˆìš©ì–´ ì²˜ë¦¬X)\n",
    "3. ì •ê·œí™” ì‚¬ì „ì€ ì°¾ì§€ ëª»í•¨ (ì •ê·œí™”í•˜ì§€ ì•Šìœ¼ë©´ vocab í¬ê¸°ê°€ ì»¤ì§€ëŠ” ë¬¸ì œ ë¿ë§Œ ì•„ë‹ˆë¼ ì˜ë¯¸ ìˆëŠ” ë‹¨ì–´ê°€ ìì£¼ ë“±ì¥í•˜ì§€ ì•ŠëŠ” í† í°ìœ¼ë¡œ ì²˜ë¦¬ë˜ì–´ ì‚­ì œë  ìˆ˜ ìˆìŒ => ì§€ê¸ˆì€ ì •ê·œí™” ì—†ì´ ì§„í–‰ í›„ ì¶”í›„ì— ë‹¤ì‹œ ê³ ë¯¼)\n",
    "4. ì˜ì–´ê°€ ë‚˜ì˜¤ë©´ ìš°ì„  í•œê¸€ë¡œ ë°”ê¿ˆ\n",
    "5. ìˆ«ìì˜ ê²½ìš° í•œê¸€ë¡œ ë°”ê¾¸ì§€ëŠ” ì•ŠìŒ\n",
    "6. 3~5ì—ì„œ ë°œìƒí•˜ëŠ” ë¬¸ì œì˜ í•µì‹¬ì€ ê²°êµ­ ë‚˜ì¤‘ì— í•™ìŠµì„ ìœ„í•´ ë“œë¬¸ í† í°ë“¤ ì‚­ì œí•  ë•Œ, ì§€ê¸ˆ ê³ ë¯¼í–ˆë˜ í† í°ë“¤ì´ ì‚­ì œê°€ ë˜ëŠ” ê²ƒì´ ë¬¸ì œì„, ìš°ì„  vocabí¬ê¸°ê°€ ë„ˆë¬´ í¬ì§€ ì•Šë‹¤ë©´ ë”°ë¡œ ì‚­ì œí•˜ì§€ ì•ŠëŠ” ê²ƒìœ¼ë¡œ ì§„í–‰ => ë’¤ì— ì „ì²˜ë¦¬ ìš”êµ¬ì‚¬í•­ì— SubwordTextEncoderë¥¼ ì‚¬ìš©í•˜ê¸° ë•Œë¬¸ì— ê²°êµ­ ì–´ëŠì •ë„ ì‚­ì œë  ìˆ˜ ë°–ì— ì—†ì—ˆìŒ (vocab sizeë¥¼ ë” í‚¤ìš°ë©´ ë” ë§ì´ ë³´ì¡´í•  ìˆ˜ ìˆìŒ; íšŒê³ ì— ì¶”ê°€ ë‚´ìš©ì´ ìˆìŠµë‹ˆë‹¤) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ì—ìŠ¤ì—ìŠ¤ë”” ì œí’ˆì€ 12ê°œì˜ ê¸°ëŠ¥ì´ ìˆìŠµë‹ˆë‹¤ ê°€ê²©ì€ 1500ì›\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def english_to_korean(word):\n",
    "    \"\"\"\n",
    "    ì˜ì–´ ë‹¨ì–´ë¥¼ í•œê¸€ ë°œìŒìœ¼ë¡œ ë³€í™˜\n",
    "    ì˜ˆ) SSD -> ì—ìŠ¤ì—ìŠ¤ë””\n",
    "    \"\"\"\n",
    "    mapping = {\n",
    "        'a': 'ì—ì´', 'b': 'ë¹„', 'c': 'ì”¨', 'd': 'ë””', 'e': 'ì´',\n",
    "        'f': 'ì—í”„', 'g': 'ì§€', 'h': 'ì—ì´ì¹˜', 'i': 'ì•„ì´',\n",
    "        'j': 'ì œì´', 'k': 'ì¼€ì´', 'l': 'ì—˜', 'm': 'ì— ',\n",
    "        'n': 'ì—”', 'o': 'ì˜¤', 'p': 'í”¼', 'q': 'í',\n",
    "        'r': 'ì•Œ', 's': 'ì—ìŠ¤', 't': 'í‹°', 'u': 'ìœ ', \n",
    "        'v': 'ë¸Œì´', 'w': 'ë”ë¸”ìœ ', 'x': 'ì—‘ìŠ¤', 'y': 'ì™€ì´', 'z': 'ì œíŠ¸'\n",
    "    }\n",
    "    return \"\".join(mapping.get(ch.lower(), ch) for ch in word)\n",
    "\n",
    "def preprocess_korean_sentence(sentence, remove_stopwords=True):\n",
    "    \"\"\"\n",
    "    1. ì•ë’¤ ê³µë°± ì œê±°\n",
    "    2. ì˜ì–´ ë‹¨ì–´ëŠ” í•œê¸€ ë°œìŒìœ¼ë¡œ ë³€í™˜ (ì˜ˆ: SSD -> ì—ìŠ¤ì—ìŠ¤ë””)\n",
    "    3. ìµœì¢…ì ìœ¼ë¡œ í•œê¸€ ìˆ«ì ì´ì™¸ì˜ ë¬¸ìë“¤ì„ ì œê±°\n",
    "    \"\"\"\n",
    "    # 1. ì•ë’¤ ê³µë°± ì œê±°\n",
    "    sentence = sentence.strip()\n",
    "       \n",
    "    # 2. ì˜ì–´ ë³€í™˜: [A-Za-z]+ íŒ¨í„´ì— ëŒ€í•´ ì˜ì–´ ë‹¨ì–´ë¥¼ í•œê¸€ ë°œìŒìœ¼ë¡œ ì¹˜í™˜\n",
    "    sentence = re.sub(r'[A-Za-z]+', lambda m: english_to_korean(m.group()), sentence)\n",
    "    \n",
    "    # 3. í•œê¸€, ìˆ«ì, ê³µë°±ì„ ì œì™¸í•œ ë¬¸ì ì œê±°\n",
    "    sentence = re.sub(r'[^ê°€-í£0-9\\s]', '', sentence)\n",
    "    \n",
    "    \n",
    "    return sentence\n",
    "\n",
    "# ì˜ˆì‹œ ì‚¬ìš©\n",
    "example_text = \"SSD ì œí’ˆì€ 12ê°œì˜ ê¸°ëŠ¥ì´ ìˆìŠµë‹ˆë‹¤. ê°€ê²©ì€ 1500ì›.\"\n",
    "print(preprocess_korean_sentence(example_text, remove_stopwords=False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'í”¼í”¼ì—˜ ì‹¬í•˜ë„¤'"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.reset_index(inplace=True, drop=True)\n",
    "\n",
    "preprocess_korean_sentence(data.loc[3,'Q'])\n",
    "#ì˜ì–´ê°€ í•œêµ­ì–´ë¡œ ì˜ ë³€í™˜ë˜ëŠ” ê²ƒì„ í™•ì¸, í˜„ì¬ ë°ì´í„°ì…‹ì—ì„œ ëŒ€ë¶€ë¶„ ì˜ì–´ëŠ” ì•½ì–´ë¡œ ì“°ì´ê¸° ë•Œë¬¸ì—\n",
    "#ì´ë ‡ê²Œ ì „ì²˜ë¦¬í•˜ëŠ”ê²Œ ì ì ˆí•˜ë‹¤ê³  íŒë‹¨"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Q</th>\n",
       "      <th>A</th>\n",
       "      <th>label</th>\n",
       "      <th>question</th>\n",
       "      <th>answer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>12ì‹œ ë•¡!</td>\n",
       "      <td>í•˜ë£¨ê°€ ë˜ ê°€ë„¤ìš”.</td>\n",
       "      <td>0</td>\n",
       "      <td>12ì‹œ ë•¡</td>\n",
       "      <td>í•˜ë£¨ê°€ ë˜ ê°€ë„¤ìš”</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1ì§€ë§ í•™êµ ë–¨ì–´ì¡Œì–´</td>\n",
       "      <td>ìœ„ë¡œí•´ ë“œë¦½ë‹ˆë‹¤.</td>\n",
       "      <td>0</td>\n",
       "      <td>1ì§€ë§ í•™êµ ë–¨ì–´ì¡Œì–´</td>\n",
       "      <td>ìœ„ë¡œí•´ ë“œë¦½ë‹ˆë‹¤</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3ë°•4ì¼ ë†€ëŸ¬ê°€ê³  ì‹¶ë‹¤</td>\n",
       "      <td>ì—¬í–‰ì€ ì–¸ì œë‚˜ ì¢‹ì£ .</td>\n",
       "      <td>0</td>\n",
       "      <td>3ë°•4ì¼ ë†€ëŸ¬ê°€ê³  ì‹¶ë‹¤</td>\n",
       "      <td>ì—¬í–‰ì€ ì–¸ì œë‚˜ ì¢‹ì£ </td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>PPL ì‹¬í•˜ë„¤</td>\n",
       "      <td>ëˆˆì‚´ì´ ì°Œí‘¸ë ¤ì§€ì£ .</td>\n",
       "      <td>0</td>\n",
       "      <td>í”¼í”¼ì—˜ ì‹¬í•˜ë„¤</td>\n",
       "      <td>ëˆˆì‚´ì´ ì°Œí‘¸ë ¤ì§€ì£ </td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>SDì¹´ë“œ ë§ê°€ì¡Œì–´</td>\n",
       "      <td>ë‹¤ì‹œ ìƒˆë¡œ ì‚¬ëŠ” ê²Œ ë§ˆìŒ í¸í•´ìš”.</td>\n",
       "      <td>0</td>\n",
       "      <td>ì—ìŠ¤ë””ì¹´ë“œ ë§ê°€ì¡Œì–´</td>\n",
       "      <td>ë‹¤ì‹œ ìƒˆë¡œ ì‚¬ëŠ” ê²Œ ë§ˆìŒ í¸í•´ìš”</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              Q                   A  label      question             answer\n",
       "0        12ì‹œ ë•¡!          í•˜ë£¨ê°€ ë˜ ê°€ë„¤ìš”.      0         12ì‹œ ë•¡          í•˜ë£¨ê°€ ë˜ ê°€ë„¤ìš”\n",
       "1   1ì§€ë§ í•™êµ ë–¨ì–´ì¡Œì–´           ìœ„ë¡œí•´ ë“œë¦½ë‹ˆë‹¤.      0   1ì§€ë§ í•™êµ ë–¨ì–´ì¡Œì–´           ìœ„ë¡œí•´ ë“œë¦½ë‹ˆë‹¤\n",
       "2  3ë°•4ì¼ ë†€ëŸ¬ê°€ê³  ì‹¶ë‹¤         ì—¬í–‰ì€ ì–¸ì œë‚˜ ì¢‹ì£ .      0  3ë°•4ì¼ ë†€ëŸ¬ê°€ê³  ì‹¶ë‹¤         ì—¬í–‰ì€ ì–¸ì œë‚˜ ì¢‹ì£ \n",
       "3       PPL ì‹¬í•˜ë„¤          ëˆˆì‚´ì´ ì°Œí‘¸ë ¤ì§€ì£ .      0       í”¼í”¼ì—˜ ì‹¬í•˜ë„¤          ëˆˆì‚´ì´ ì°Œí‘¸ë ¤ì§€ì£ \n",
       "4     SDì¹´ë“œ ë§ê°€ì¡Œì–´  ë‹¤ì‹œ ìƒˆë¡œ ì‚¬ëŠ” ê²Œ ë§ˆìŒ í¸í•´ìš”.      0    ì—ìŠ¤ë””ì¹´ë“œ ë§ê°€ì¡Œì–´  ë‹¤ì‹œ ìƒˆë¡œ ì‚¬ëŠ” ê²Œ ë§ˆìŒ í¸í•´ìš”"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for i in range(len(data)):\n",
    "   data.loc[i,'question'] = preprocess_korean_sentence(data.loc[i,'Q'])\n",
    "   data.loc[i,'answer'] = preprocess_korean_sentence(data.loc[i,'A'])\n",
    "\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 3. SubwordTextEncoder ì‚¬ìš©í•˜ê¸°\n",
    "í•œêµ­ì–´ ë°ì´í„°ëŠ” í˜•íƒœì†Œ ë¶„ì„ê¸°ë¥¼ ì‚¬ìš©í•˜ì—¬ í† í¬ë‚˜ì´ì§•ì„ í•´ì•¼ í•œë‹¤ê³  ë§ì€ ë¶„ì´ ì•Œê³  ìˆìŠµë‹ˆë‹¤. í•˜ì§€ë§Œ ì—¬ê¸°ì„œëŠ” í˜•íƒœì†Œ ë¶„ì„ê¸°ê°€ ì•„ë‹Œ ìœ„ ì‹¤ìŠµì—ì„œ ì‚¬ìš©í–ˆë˜ ë‚´ë¶€ ë‹¨ì–´ í† í¬ë‚˜ì´ì €ì¸ SubwordTextEncoderë¥¼ ê·¸ëŒ€ë¡œ ì‚¬ìš©í•´ë³´ì„¸ìš”."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "questions = data['question']\n",
    "answers = data['answer']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ì‚´ì§ ì˜¤ë˜ ê±¸ë¦´ ìˆ˜ ìˆì–´ìš”. ìŠ¤íŠ¸ë ˆì¹­ í•œ ë²ˆ í•´ë³¼ê¹Œìš”? ğŸ‘\n",
      "ìŠ=3 \n"
     ]
    }
   ],
   "source": [
    "import tensorflow_datasets as tfds\n",
    "print(\"ì‚´ì§ ì˜¤ë˜ ê±¸ë¦´ ìˆ˜ ìˆì–´ìš”. ìŠ¤íŠ¸ë ˆì¹­ í•œ ë²ˆ í•´ë³¼ê¹Œìš”? ğŸ‘\")\n",
    "\n",
    "# ì§ˆë¬¸ê³¼ ë‹µë³€ ë°ì´í„°ì…‹ì— ëŒ€í•´ì„œ Vocabulary ìƒì„±\n",
    "tokenizer = tfds.deprecated.text.SubwordTextEncoder.build_from_corpus(questions + answers, target_vocab_size=2**13)\n",
    "print(\"ìŠ=3 \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ìŠ=3\n"
     ]
    }
   ],
   "source": [
    "# ì‹œì‘ í† í°ê³¼ ì¢…ë£Œ í† í°ì— ê³ ìœ í•œ ì •ìˆ˜ë¥¼ ë¶€ì—¬í•©ë‹ˆë‹¤.\n",
    "START_TOKEN, END_TOKEN = [tokenizer.vocab_size], [tokenizer.vocab_size + 1]\n",
    "print(\"ìŠ=3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "START_TOKENì˜ ë²ˆí˜¸ : [8108]\n",
      "END_TOKENì˜ ë²ˆí˜¸ : [8109]\n"
     ]
    }
   ],
   "source": [
    "print('START_TOKENì˜ ë²ˆí˜¸ :' ,[tokenizer.vocab_size])\n",
    "print('END_TOKENì˜ ë²ˆí˜¸ :' ,[tokenizer.vocab_size + 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8110\n"
     ]
    }
   ],
   "source": [
    "# ì‹œì‘ í† í°ê³¼ ì¢…ë£Œ í† í°ì„ ê³ ë ¤í•˜ì—¬ +2ë¥¼ í•˜ì—¬ ë‹¨ì–´ì¥ì˜ í¬ê¸°ë¥¼ ì‚°ì •í•©ë‹ˆë‹¤.\n",
    "VOCAB_SIZE = tokenizer.vocab_size + 2\n",
    "print(VOCAB_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ì •ìˆ˜ ì¸ì½”ë”© í›„ì˜ 21ë²ˆì§¸ ì§ˆë¬¸ ìƒ˜í”Œ: [5240, 1087, 134, 524, 60]\n",
      "ì •ìˆ˜ ì¸ì½”ë”© í›„ì˜ 21ë²ˆì§¸ ë‹µë³€ ìƒ˜í”Œ: [1782, 6894, 6044, 7884, 61, 2789, 750]\n"
     ]
    }
   ],
   "source": [
    "# ì„ì˜ì˜ 22ë²ˆì§¸ ìƒ˜í”Œì— ëŒ€í•´ì„œ ì •ìˆ˜ ì¸ì½”ë”© ì‘ì—…ì„ ìˆ˜í–‰.\n",
    "# ê° í† í°ì„ ê³ ìœ í•œ ì •ìˆ˜ë¡œ ë³€í™˜\n",
    "print('ì •ìˆ˜ ì¸ì½”ë”© í›„ì˜ 21ë²ˆì§¸ ì§ˆë¬¸ ìƒ˜í”Œ: {}'.format(tokenizer.encode(questions[21])))\n",
    "print('ì •ìˆ˜ ì¸ì½”ë”© í›„ì˜ 21ë²ˆì§¸ ë‹µë³€ ìƒ˜í”Œ: {}'.format(tokenizer.encode(answers[21])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ìŠ=3\n"
     ]
    }
   ],
   "source": [
    "# ìƒ˜í”Œì˜ ìµœëŒ€ í—ˆìš© ê¸¸ì´ ë˜ëŠ” íŒ¨ë”© í›„ì˜ ìµœì¢… ê¸¸ì´\n",
    "MAX_LENGTH = 40\n",
    "\n",
    "# ì •ìˆ˜ ì¸ì½”ë”©, ìµœëŒ€ ê¸¸ì´ë¥¼ ì´ˆê³¼í•˜ëŠ” ìƒ˜í”Œ ì œê±°, íŒ¨ë”©\n",
    "def tokenize_and_filter(inputs, outputs):\n",
    "  tokenized_inputs, tokenized_outputs = [], []\n",
    "  \n",
    "  for (sentence1, sentence2) in zip(inputs, outputs):\n",
    "    # ì •ìˆ˜ ì¸ì½”ë”© ê³¼ì •ì—ì„œ ì‹œì‘ í† í°ê³¼ ì¢…ë£Œ í† í°ì„ ì¶”ê°€\n",
    "    sentence1 = START_TOKEN + tokenizer.encode(sentence1) + END_TOKEN\n",
    "    sentence2 = START_TOKEN + tokenizer.encode(sentence2) + END_TOKEN\n",
    "\n",
    "    # ìµœëŒ€ ê¸¸ì´ 40 ì´í•˜ì¸ ê²½ìš°ì—ë§Œ ë°ì´í„°ì…‹ìœ¼ë¡œ í—ˆìš©\n",
    "    if len(sentence1) <= MAX_LENGTH and len(sentence2) <= MAX_LENGTH:\n",
    "      tokenized_inputs.append(sentence1)\n",
    "      tokenized_outputs.append(sentence2)\n",
    "  \n",
    "  # ìµœëŒ€ ê¸¸ì´ 40ìœ¼ë¡œ ëª¨ë“  ë°ì´í„°ì…‹ì„ íŒ¨ë”©\n",
    "  tokenized_inputs = tf.keras.preprocessing.sequence.pad_sequences(\n",
    "      tokenized_inputs, maxlen=MAX_LENGTH, padding='post')\n",
    "  tokenized_outputs = tf.keras.preprocessing.sequence.pad_sequences(\n",
    "      tokenized_outputs, maxlen=MAX_LENGTH, padding='post')\n",
    "  \n",
    "  return tokenized_inputs, tokenized_outputs\n",
    "print(\"ìŠ=3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ë‹¨ì–´ì¥ì˜ í¬ê¸° : 8110\n",
      "í•„í„°ë§ í›„ì˜ ì§ˆë¬¸ ìƒ˜í”Œ ê°œìˆ˜: 7779\n",
      "í•„í„°ë§ í›„ì˜ ë‹µë³€ ìƒ˜í”Œ ê°œìˆ˜: 7779\n"
     ]
    }
   ],
   "source": [
    "questions, answers = tokenize_and_filter(questions, answers)\n",
    "print('ë‹¨ì–´ì¥ì˜ í¬ê¸° :',(VOCAB_SIZE))\n",
    "print('í•„í„°ë§ í›„ì˜ ì§ˆë¬¸ ìƒ˜í”Œ ê°œìˆ˜: {}'.format(len(questions)))\n",
    "print('í•„í„°ë§ í›„ì˜ ë‹µë³€ ìƒ˜í”Œ ê°œìˆ˜: {}'.format(len(answers)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ìŠ=3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-14 13:23:29.079537: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] could not open file to read NUMA node: /sys/bus/pci/devices/0000:08:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2025-02-14 13:23:29.101952: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] could not open file to read NUMA node: /sys/bus/pci/devices/0000:08:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2025-02-14 13:23:29.102008: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] could not open file to read NUMA node: /sys/bus/pci/devices/0000:08:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2025-02-14 13:23:29.105932: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] could not open file to read NUMA node: /sys/bus/pci/devices/0000:08:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2025-02-14 13:23:29.105972: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] could not open file to read NUMA node: /sys/bus/pci/devices/0000:08:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2025-02-14 13:23:29.106001: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] could not open file to read NUMA node: /sys/bus/pci/devices/0000:08:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2025-02-14 13:23:29.246691: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] could not open file to read NUMA node: /sys/bus/pci/devices/0000:08:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2025-02-14 13:23:29.246760: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] could not open file to read NUMA node: /sys/bus/pci/devices/0000:08:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2025-02-14 13:23:29.246767: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1726] Could not identify NUMA node of platform GPU id 0, defaulting to 0.  Your kernel may not have been built with NUMA support.\n",
      "2025-02-14 13:23:29.246803: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] could not open file to read NUMA node: /sys/bus/pci/devices/0000:08:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2025-02-14 13:23:29.246829: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 9711 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3060, pci bus id: 0000:08:00.0, compute capability: 8.6\n"
     ]
    }
   ],
   "source": [
    "BATCH_SIZE = 64\n",
    "BUFFER_SIZE = 20000\n",
    "\n",
    "# ë””ì½”ë”ëŠ” ì´ì „ì˜ targetì„ ë‹¤ìŒì˜ inputìœ¼ë¡œ ì‚¬ìš©í•©ë‹ˆë‹¤.\n",
    "# ì´ì— ë”°ë¼ outputsì—ì„œëŠ” START_TOKENì„ ì œê±°í•˜ê² ìŠµë‹ˆë‹¤.\n",
    "dataset = tf.data.Dataset.from_tensor_slices((\n",
    "    {\n",
    "        'inputs': questions,\n",
    "        'dec_inputs': answers[:, :-1]\n",
    "    },\n",
    "    {\n",
    "        'outputs': answers[:, 1:]\n",
    "    },\n",
    "))\n",
    "\n",
    "dataset = dataset.cache()\n",
    "dataset = dataset.shuffle(BUFFER_SIZE)\n",
    "dataset = dataset.batch(BATCH_SIZE)\n",
    "dataset = dataset.prefetch(tf.data.experimental.AUTOTUNE)\n",
    "print(\"ìŠ=3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 4. ëª¨ë¸ êµ¬ì„±í•˜ê¸°\n",
    "ìœ„ ì‹¤ìŠµ ë‚´ìš©ì„ ì°¸ê³ í•˜ì—¬ íŠ¸ëœìŠ¤í¬ë¨¸ ëª¨ë¸ì„ êµ¬í˜„í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ìŠ=3\n"
     ]
    }
   ],
   "source": [
    "# í¬ì§€ì…”ë„ ì¸ì½”ë”© ë ˆì´ì–´\n",
    "class PositionalEncoding(tf.keras.layers.Layer):\n",
    "\n",
    "  def __init__(self, position, d_model):\n",
    "    super(PositionalEncoding, self).__init__()\n",
    "    self.pos_encoding = self.positional_encoding(position, d_model)\n",
    "\n",
    "  def get_angles(self, position, i, d_model):\n",
    "    angles = 1 / tf.pow(10000, (2 * (i // 2)) / tf.cast(d_model, tf.float32))\n",
    "    return position * angles\n",
    "\n",
    "  def positional_encoding(self, position, d_model):\n",
    "    # ê°ë„ ë°°ì—´ ìƒì„±\n",
    "    angle_rads = self.get_angles(\n",
    "        position=tf.range(position, dtype=tf.float32)[:, tf.newaxis],\n",
    "        i=tf.range(d_model, dtype=tf.float32)[tf.newaxis, :],\n",
    "        d_model=d_model)\n",
    "\n",
    "    # ë°°ì—´ì˜ ì§ìˆ˜ ì¸ë±ìŠ¤ì—ëŠ” sin í•¨ìˆ˜ ì ìš©\n",
    "    sines = tf.math.sin(angle_rads[:, 0::2])\n",
    "    # ë°°ì—´ì˜ í™€ìˆ˜ ì¸ë±ìŠ¤ì—ëŠ” cosine í•¨ìˆ˜ ì ìš©\n",
    "    cosines = tf.math.cos(angle_rads[:, 1::2])\n",
    "\n",
    "    # sinê³¼ cosineì´ êµì°¨ë˜ë„ë¡ ì¬ë°°ì—´\n",
    "    pos_encoding = tf.stack([sines, cosines], axis=0)\n",
    "    pos_encoding = tf.transpose(pos_encoding,[1, 2, 0]) \n",
    "    pos_encoding = tf.reshape(pos_encoding, [position, d_model])\n",
    "\n",
    "    pos_encoding = pos_encoding[tf.newaxis, ...]\n",
    "    return tf.cast(pos_encoding, tf.float32)\n",
    "\n",
    "  def call(self, inputs):\n",
    "    return inputs + self.pos_encoding[:, :tf.shape(inputs)[1], :]\n",
    "\n",
    "print(\"ìŠ=3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ìŠ=3\n"
     ]
    }
   ],
   "source": [
    "# ìŠ¤ì¼€ì¼ë“œ ë‹· í”„ë¡œë•íŠ¸ ì–´í…ì…˜ í•¨ìˆ˜\n",
    "def scaled_dot_product_attention(query, key, value, mask):\n",
    "  # ì–´í…ì…˜ ê°€ì¤‘ì¹˜ëŠ” Qì™€ Kì˜ ë‹· í”„ë¡œë•íŠ¸\n",
    "  matmul_qk = tf.matmul(query, key, transpose_b=True)\n",
    "\n",
    "  # ê°€ì¤‘ì¹˜ë¥¼ ì •ê·œí™”\n",
    "  depth = tf.cast(tf.shape(key)[-1], tf.float32)\n",
    "  logits = matmul_qk / tf.math.sqrt(depth)\n",
    "\n",
    "  # íŒ¨ë”©ì— ë§ˆìŠ¤í¬ ì¶”ê°€\n",
    "  if mask is not None:\n",
    "    logits += (mask * -1e9)\n",
    "\n",
    "  # softmaxì ìš©\n",
    "  attention_weights = tf.nn.softmax(logits, axis=-1)\n",
    "\n",
    "  # ìµœì¢… ì–´í…ì…˜ì€ ê°€ì¤‘ì¹˜ì™€ Vì˜ ë‹· í”„ë¡œë•íŠ¸\n",
    "  output = tf.matmul(attention_weights, value)\n",
    "  return output\n",
    "\n",
    "print(\"ìŠ=3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ìŠ=3\n"
     ]
    }
   ],
   "source": [
    "class MultiHeadAttention(tf.keras.layers.Layer):\n",
    "\n",
    "  def __init__(self, d_model, num_heads, name=\"multi_head_attention\"):\n",
    "    super(MultiHeadAttention, self).__init__(name=name)\n",
    "    self.num_heads = num_heads\n",
    "    self.d_model = d_model\n",
    "\n",
    "    assert d_model % self.num_heads == 0 # ë‚˜ëˆ„ì–´ ë–¨ì–´ì§€ì§€ ì•Šìœ¼ë©´ ìˆ˜í–‰ ë¶ˆê°€\n",
    "\n",
    "    self.depth = d_model // self.num_heads # ì—¬ê¸°ê°€ Q,K,Vê°€ muti-headë¡œ ë‚˜ë‰˜ì—ˆì„ ë•Œì˜ ì°¨ì› ìˆ˜\n",
    "\n",
    "    self.query_dense = tf.keras.layers.Dense(units=d_model)\n",
    "    self.key_dense = tf.keras.layers.Dense(units=d_model)\n",
    "    self.value_dense = tf.keras.layers.Dense(units=d_model) # ì•„ì§ ì–´í…ì…˜ ê³„ì‚° ì „ í˜•íƒœ\n",
    "\n",
    "    self.dense = tf.keras.layers.Dense(units=d_model)# ë‹¤ìŒ ë ˆì´ì–´ë¡œ ë„˜ì–´ê°€ëŠ” ìµœì¢… ë ˆì´ì–´ \n",
    "\n",
    "  def split_heads(self, inputs, batch_size):\n",
    "    inputs = tf.reshape(\n",
    "        inputs, shape=(batch_size, -1, self.num_heads, self.depth))# -1ì€ ì…ë ¥ ë¬¸ì¥ì˜ ê¸¸ì´\n",
    "    return tf.transpose(inputs, perm=[0, 2, 1, 3])# (batch_size, num_heads, seq_len, depth)\n",
    "\n",
    "  def call(self, inputs):\n",
    "    query, key, value, mask = inputs['query'], inputs['key'], inputs[\n",
    "        'value'], inputs['mask']\n",
    "    batch_size = tf.shape(query)[0]\n",
    "\n",
    "    # Q, K, Vì— ê°ê° Denseë¥¼ ì ìš©í•©ë‹ˆë‹¤\n",
    "    query = self.query_dense(query)\n",
    "    key = self.key_dense(key)\n",
    "    value = self.value_dense(value)\n",
    "\n",
    "    # ë³‘ë ¬ ì—°ì‚°ì„ ìœ„í•œ ë¨¸ë¦¬ë¥¼ ì—¬ëŸ¬ ê°œ ë§Œë“­ë‹ˆë‹¤\n",
    "    query = self.split_heads(query, tf.shape(query)[0])\n",
    "    key = self.split_heads(key, tf.shape(key)[0])\n",
    "    value = self.split_heads(value, tf.shape(value)[0])\n",
    "\n",
    "    # ìŠ¤ì¼€ì¼ë“œ ë‹· í”„ë¡œë•íŠ¸ ì–´í…ì…˜ í•¨ìˆ˜\n",
    "    scaled_attention = scaled_dot_product_attention(query, key, value, mask)\n",
    "\n",
    "    scaled_attention = tf.transpose(scaled_attention, perm=[0, 2, 1, 3])\n",
    "\n",
    "    # ì–´í…ì…˜ ì—°ì‚° í›„ì— ê° ê²°ê³¼ë¥¼ ë‹¤ì‹œ ì—°ê²°(concatenate)í•©ë‹ˆë‹¤\n",
    "    concat_attention = tf.reshape(scaled_attention,\n",
    "                                  (batch_size, -1, self.d_model))# (batch_size, seq_len, d_model)\n",
    "\n",
    "    # ìµœì¢… ê²°ê³¼ì—ë„ Denseë¥¼ í•œ ë²ˆ ë” ì ìš©í•©ë‹ˆë‹¤\n",
    "    outputs = self.dense(concat_attention)\n",
    "\n",
    "    return outputs\n",
    "print(\"ìŠ=3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ìŠ=3\n"
     ]
    }
   ],
   "source": [
    "def create_padding_mask(x):\n",
    "  mask = tf.cast(tf.math.equal(x, 0), tf.float32)\n",
    "  # (batch_size, 1, 1, sequence length)\n",
    "  return mask[:, tf.newaxis, tf.newaxis , : ]\n",
    "print(\"ìŠ=3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ìŠ=3\n"
     ]
    }
   ],
   "source": [
    "def create_look_ahead_mask(x):\n",
    "  seq_len = tf.shape(x)[1]\n",
    "  look_ahead_mask = 1 - tf.linalg.band_part(tf.ones((seq_len, seq_len)), -1, 0)\n",
    "  padding_mask = create_padding_mask(x)\n",
    "  return tf.maximum(look_ahead_mask, padding_mask)\n",
    "print(\"ìŠ=3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ìŠ=3\n"
     ]
    }
   ],
   "source": [
    "# ì¸ì½”ë” í•˜ë‚˜ì˜ ë ˆì´ì–´ë¥¼ í•¨ìˆ˜ë¡œ êµ¬í˜„.\n",
    "# ì´ í•˜ë‚˜ì˜ ë ˆì´ì–´ ì•ˆì—ëŠ” ë‘ ê°œì˜ ì„œë¸Œ ë ˆì´ì–´ê°€ ì¡´ì¬í•©ë‹ˆë‹¤.\n",
    "def encoder_layer(units, d_model, num_heads, dropout, name=\"encoder_layer\"):\n",
    "  inputs = tf.keras.Input(shape=(None, d_model), name=\"inputs\")\n",
    "\n",
    "  # íŒ¨ë”© ë§ˆìŠ¤í¬ ì‚¬ìš©\n",
    "  padding_mask = tf.keras.Input(shape=(1, 1, None), name=\"padding_mask\")\n",
    "\n",
    "  # ì²« ë²ˆì§¸ ì„œë¸Œ ë ˆì´ì–´ : ë©€í‹° í—¤ë“œ ì–´í…ì…˜ ìˆ˜í–‰ (ì…€í”„ ì–´í…ì…˜)\n",
    "  attention = MultiHeadAttention(\n",
    "      d_model, num_heads, name=\"attention\")({\n",
    "          'query': inputs,\n",
    "          'key': inputs,\n",
    "          'value': inputs,\n",
    "          'mask': padding_mask\n",
    "      })\n",
    "\n",
    "  # ì–´í…ì…˜ì˜ ê²°ê³¼ëŠ” Dropoutê³¼ Layer Normalizationì´ë¼ëŠ” í›ˆë ¨ì„ ë•ëŠ” í…Œí¬ë‹‰ì„ ìˆ˜í–‰\n",
    "  attention = tf.keras.layers.Dropout(rate=dropout)(attention)\n",
    "  attention = tf.keras.layers.LayerNormalization(\n",
    "      epsilon=1e-6)(inputs + attention)\n",
    "\n",
    "  # ë‘ ë²ˆì§¸ ì„œë¸Œ ë ˆì´ì–´ : 2ê°œì˜ ì™„ì „ì—°ê²°ì¸µ\n",
    "  outputs = tf.keras.layers.Dense(units=units, activation='relu')(attention)\n",
    "  outputs = tf.keras.layers.Dense(units=d_model)(outputs)\n",
    "\n",
    "  # ì™„ì „ì—°ê²°ì¸µì˜ ê²°ê³¼ëŠ” Dropoutê³¼ LayerNormalizationì´ë¼ëŠ” í›ˆë ¨ì„ ë•ëŠ” í…Œí¬ë‹‰ì„ ìˆ˜í–‰\n",
    "  outputs = tf.keras.layers.Dropout(rate=dropout)(outputs)\n",
    "  outputs = tf.keras.layers.LayerNormalization(\n",
    "      epsilon=1e-6)(attention + outputs)\n",
    "\n",
    "  return tf.keras.Model(\n",
    "      inputs=[inputs, padding_mask], outputs=outputs, name=name)\n",
    "print(\"ìŠ=3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ìŠ=3\n"
     ]
    }
   ],
   "source": [
    "def encoder(vocab_size,\n",
    "            num_layers,\n",
    "            units,\n",
    "            d_model,\n",
    "            num_heads,\n",
    "            dropout,\n",
    "            name=\"encoder\"):\n",
    "  inputs = tf.keras.Input(shape=(None,), name=\"inputs\")\n",
    "\n",
    "  # íŒ¨ë”© ë§ˆìŠ¤í¬ ì‚¬ìš©\n",
    "  padding_mask = tf.keras.Input(shape=(1, 1, None), name=\"padding_mask\")\n",
    "\n",
    "  # ì„ë² ë”© ë ˆì´ì–´\n",
    "  embeddings = tf.keras.layers.Embedding(vocab_size, d_model)(inputs)\n",
    "  embeddings *= tf.math.sqrt(tf.cast(d_model, tf.float32))\n",
    "\n",
    "  # í¬ì§€ì…”ë„ ì¸ì½”ë”©\n",
    "  embeddings = PositionalEncoding(vocab_size, d_model)(embeddings)\n",
    "\n",
    "  outputs = tf.keras.layers.Dropout(rate=dropout)(embeddings)\n",
    "\n",
    "  # num_layersë§Œí¼ ìŒ“ì•„ì˜¬ë¦° ì¸ì½”ë”ì˜ ì¸µ.\n",
    "  for i in range(num_layers):\n",
    "    outputs = encoder_layer(\n",
    "        units=units,\n",
    "        d_model=d_model,\n",
    "        num_heads=num_heads,\n",
    "        dropout=dropout,\n",
    "        name=\"encoder_layer_{}\".format(i),\n",
    "    )([outputs, padding_mask])\n",
    "\n",
    "  return tf.keras.Model(\n",
    "      inputs=[inputs, padding_mask], outputs=outputs, name=name)\n",
    "print(\"ìŠ=3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ìŠ=3\n"
     ]
    }
   ],
   "source": [
    "# ë””ì½”ë” í•˜ë‚˜ì˜ ë ˆì´ì–´ë¥¼ í•¨ìˆ˜ë¡œ êµ¬í˜„.\n",
    "# ì´ í•˜ë‚˜ì˜ ë ˆì´ì–´ ì•ˆì—ëŠ” ì„¸ ê°œì˜ ì„œë¸Œ ë ˆì´ì–´ê°€ ì¡´ì¬í•©ë‹ˆë‹¤.\n",
    "def decoder_layer(units, d_model, num_heads, dropout, name=\"decoder_layer\"):\n",
    "  inputs = tf.keras.Input(shape=(None, d_model), name=\"inputs\")\n",
    "  enc_outputs = tf.keras.Input(shape=(None, d_model), name=\"encoder_outputs\")\n",
    "  look_ahead_mask = tf.keras.Input(\n",
    "      shape=(1, None, None), name=\"look_ahead_mask\")\n",
    "  padding_mask = tf.keras.Input(shape=(1, 1, None), name='padding_mask')\n",
    "\n",
    "  # ì²« ë²ˆì§¸ ì„œë¸Œ ë ˆì´ì–´ : ë©€í‹° í—¤ë“œ ì–´í…ì…˜ ìˆ˜í–‰ (ì…€í”„ ì–´í…ì…˜)\n",
    "  attention1 = MultiHeadAttention(\n",
    "      d_model, num_heads, name=\"attention_1\")(inputs={\n",
    "          'query': inputs,\n",
    "          'key': inputs,\n",
    "          'value': inputs,\n",
    "          'mask': look_ahead_mask\n",
    "      })\n",
    "\n",
    "  # ë©€í‹° í—¤ë“œ ì–´í…ì…˜ì˜ ê²°ê³¼ëŠ” LayerNormalizationì´ë¼ëŠ” í›ˆë ¨ì„ ë•ëŠ” í…Œí¬ë‹‰ì„ ìˆ˜í–‰\n",
    "  attention1 = tf.keras.layers.LayerNormalization(\n",
    "      epsilon=1e-6)(attention1 + inputs)\n",
    "\n",
    "  # ë‘ ë²ˆì§¸ ì„œë¸Œ ë ˆì´ì–´ : ë§ˆìŠ¤í¬ë“œ ë©€í‹° í—¤ë“œ ì–´í…ì…˜ ìˆ˜í–‰ (ì¸ì½”ë”-ë””ì½”ë” ì–´í…ì…˜)\n",
    "  attention2 = MultiHeadAttention(\n",
    "      d_model, num_heads, name=\"attention_2\")(inputs={\n",
    "          'query': attention1,\n",
    "          'key': enc_outputs,\n",
    "          'value': enc_outputs,\n",
    "          'mask': padding_mask\n",
    "      })\n",
    "\n",
    "  # ë§ˆìŠ¤í¬ë“œ ë©€í‹° í—¤ë“œ ì–´í…ì…˜ì˜ ê²°ê³¼ëŠ”\n",
    "  # Dropoutê³¼ LayerNormalizationì´ë¼ëŠ” í›ˆë ¨ì„ ë•ëŠ” í…Œí¬ë‹‰ì„ ìˆ˜í–‰\n",
    "  attention2 = tf.keras.layers.Dropout(rate=dropout)(attention2)\n",
    "  attention2 = tf.keras.layers.LayerNormalization(\n",
    "      epsilon=1e-6)(attention2 + attention1)\n",
    "\n",
    "  # ì„¸ ë²ˆì§¸ ì„œë¸Œ ë ˆì´ì–´ : 2ê°œì˜ ì™„ì „ì—°ê²°ì¸µ\n",
    "  outputs = tf.keras.layers.Dense(units=units, activation='relu')(attention2)\n",
    "  outputs = tf.keras.layers.Dense(units=d_model)(outputs)\n",
    "\n",
    "  # ì™„ì „ì—°ê²°ì¸µì˜ ê²°ê³¼ëŠ” Dropoutê³¼ LayerNormalization ìˆ˜í–‰\n",
    "  outputs = tf.keras.layers.Dropout(rate=dropout)(outputs)\n",
    "  outputs = tf.keras.layers.LayerNormalization(\n",
    "      epsilon=1e-6)(outputs + attention2)\n",
    "\n",
    "  return tf.keras.Model(\n",
    "      inputs=[inputs, enc_outputs, look_ahead_mask, padding_mask],\n",
    "      outputs=outputs,\n",
    "      name=name)\n",
    "print(\"ìŠ=3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ìŠ=3\n"
     ]
    }
   ],
   "source": [
    "def decoder(vocab_size,\n",
    "            num_layers,\n",
    "            units,\n",
    "            d_model,\n",
    "            num_heads,\n",
    "            dropout,\n",
    "            name='decoder'):\n",
    "  inputs = tf.keras.Input(shape=(None,), name='inputs')\n",
    "  enc_outputs = tf.keras.Input(shape=(None, d_model), name='encoder_outputs')\n",
    "  look_ahead_mask = tf.keras.Input(\n",
    "      shape=(1, None, None), name='look_ahead_mask')\n",
    "\n",
    "  # íŒ¨ë”© ë§ˆìŠ¤í¬\n",
    "  padding_mask = tf.keras.Input(shape=(1, 1, None), name='padding_mask')\n",
    "  \n",
    "  # ì„ë² ë”© ë ˆì´ì–´\n",
    "  embeddings = tf.keras.layers.Embedding(vocab_size, d_model)(inputs)\n",
    "  embeddings *= tf.math.sqrt(tf.cast(d_model, tf.float32))\n",
    "\n",
    "  # í¬ì§€ì…”ë„ ì¸ì½”ë”©\n",
    "  embeddings = PositionalEncoding(vocab_size, d_model)(embeddings)\n",
    "\n",
    "  # Dropoutì´ë¼ëŠ” í›ˆë ¨ì„ ë•ëŠ” í…Œí¬ë‹‰ì„ ìˆ˜í–‰\n",
    "  outputs = tf.keras.layers.Dropout(rate=dropout)(embeddings)\n",
    "\n",
    "  for i in range(num_layers):\n",
    "    outputs = decoder_layer(\n",
    "        units=units,\n",
    "        d_model=d_model,\n",
    "        num_heads=num_heads,\n",
    "        dropout=dropout,\n",
    "        name='decoder_layer_{}'.format(i),\n",
    "    )(inputs=[outputs, enc_outputs, look_ahead_mask, padding_mask])\n",
    "\n",
    "  return tf.keras.Model(\n",
    "      inputs=[inputs, enc_outputs, look_ahead_mask, padding_mask],\n",
    "      outputs=outputs,\n",
    "      name=name)\n",
    "print(\"ìŠ=3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ìŠ=3\n"
     ]
    }
   ],
   "source": [
    "def transformer(vocab_size,\n",
    "                num_layers,\n",
    "                units,\n",
    "                d_model,\n",
    "                num_heads,\n",
    "                dropout,\n",
    "                name=\"transformer\"):\n",
    "  inputs = tf.keras.Input(shape=(None,), name=\"inputs\")\n",
    "  dec_inputs = tf.keras.Input(shape=(None,), name=\"dec_inputs\")\n",
    "\n",
    "  # ì¸ì½”ë”ì—ì„œ íŒ¨ë”©ì„ ìœ„í•œ ë§ˆìŠ¤í¬\n",
    "  enc_padding_mask = tf.keras.layers.Lambda(\n",
    "      create_padding_mask, output_shape=(1, 1, None),\n",
    "      name='enc_padding_mask')(inputs)\n",
    "\n",
    "  # ë””ì½”ë”ì—ì„œ ë¯¸ë˜ì˜ í† í°ì„ ë§ˆìŠ¤í¬ í•˜ê¸° ìœ„í•´ì„œ ì‚¬ìš©í•©ë‹ˆë‹¤.\n",
    "  # ë‚´ë¶€ì ìœ¼ë¡œ íŒ¨ë”© ë§ˆìŠ¤í¬ë„ í¬í•¨ë˜ì–´ì ¸ ìˆìŠµë‹ˆë‹¤.\n",
    "  look_ahead_mask = tf.keras.layers.Lambda(\n",
    "      create_look_ahead_mask,\n",
    "      output_shape=(1, None, None),\n",
    "      name='look_ahead_mask')(dec_inputs)\n",
    "\n",
    "  # ë‘ ë²ˆì§¸ ì–´í…ì…˜ ë¸”ë¡ì—ì„œ ì¸ì½”ë”ì˜ ë²¡í„°ë“¤ì„ ë§ˆìŠ¤í‚¹\n",
    "  # ë””ì½”ë”ì—ì„œ íŒ¨ë”©ì„ ìœ„í•œ ë§ˆìŠ¤í¬\n",
    "  dec_padding_mask = tf.keras.layers.Lambda(\n",
    "      create_padding_mask, output_shape=(1, 1, None),\n",
    "      name='dec_padding_mask')(inputs)\n",
    "\n",
    "  # ì¸ì½”ë”\n",
    "  enc_outputs = encoder(\n",
    "      vocab_size=vocab_size,\n",
    "      num_layers=num_layers,\n",
    "      units=units,\n",
    "      d_model=d_model,\n",
    "      num_heads=num_heads,\n",
    "      dropout=dropout,\n",
    "  )(inputs=[inputs, enc_padding_mask])\n",
    "\n",
    "  # ë””ì½”ë”\n",
    "  dec_outputs = decoder(\n",
    "      vocab_size=vocab_size,\n",
    "      num_layers=num_layers,\n",
    "      units=units,\n",
    "      d_model=d_model,\n",
    "      num_heads=num_heads,\n",
    "      dropout=dropout,\n",
    "  )(inputs=[dec_inputs, enc_outputs, look_ahead_mask, dec_padding_mask])\n",
    "\n",
    "  # ì™„ì „ì—°ê²°ì¸µ\n",
    "  outputs = tf.keras.layers.Dense(units=vocab_size, name=\"outputs\")(dec_outputs)\n",
    "\n",
    "  return tf.keras.Model(inputs=[inputs, dec_inputs], outputs=outputs, name=name)\n",
    "print(\"ìŠ=3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"transformer\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                Output Shape                 Param #   Connected to                  \n",
      "==================================================================================================\n",
      " inputs (InputLayer)         [(None, None)]               0         []                            \n",
      "                                                                                                  \n",
      " dec_inputs (InputLayer)     [(None, None)]               0         []                            \n",
      "                                                                                                  \n",
      " enc_padding_mask (Lambda)   (None, 1, 1, None)           0         ['inputs[0][0]']              \n",
      "                                                                                                  \n",
      " encoder (Functional)        (None, None, 256)            3130368   ['inputs[0][0]',              \n",
      "                                                                     'enc_padding_mask[0][0]']    \n",
      "                                                                                                  \n",
      " look_ahead_mask (Lambda)    (None, 1, None, None)        0         ['dec_inputs[0][0]']          \n",
      "                                                                                                  \n",
      " dec_padding_mask (Lambda)   (None, 1, 1, None)           0         ['inputs[0][0]']              \n",
      "                                                                                                  \n",
      " decoder (Functional)        (None, None, 256)            3657728   ['dec_inputs[0][0]',          \n",
      "                                                                     'encoder[0][0]',             \n",
      "                                                                     'look_ahead_mask[0][0]',     \n",
      "                                                                     'dec_padding_mask[0][0]']    \n",
      "                                                                                                  \n",
      " outputs (Dense)             (None, None, 8110)           2084270   ['decoder[0][0]']             \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 8872366 (33.85 MB)\n",
      "Trainable params: 8872366 (33.85 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "tf.keras.backend.clear_session()\n",
    "\n",
    "# í•˜ì´í¼íŒŒë¼ë¯¸í„°\n",
    "NUM_LAYERS = 2 # ì¸ì½”ë”ì™€ ë””ì½”ë”ì˜ ì¸µì˜ ê°œìˆ˜\n",
    "D_MODEL = 256 # ì¸ì½”ë”ì™€ ë””ì½”ë” ë‚´ë¶€ì˜ ì…, ì¶œë ¥ì˜ ê³ ì • ì°¨ì›\n",
    "NUM_HEADS = 8 # ë©€í‹° í—¤ë“œ ì–´í…ì…˜ì—ì„œì˜ í—¤ë“œ ìˆ˜ \n",
    "UNITS = 512 # í”¼ë“œ í¬ì›Œë“œ ì‹ ê²½ë§ì˜ ì€ë‹‰ì¸µì˜ í¬ê¸°\n",
    "DROPOUT = 0.1 # ë“œë¡­ì•„ì›ƒì˜ ë¹„ìœ¨\n",
    "\n",
    "model = transformer(\n",
    "    vocab_size=VOCAB_SIZE,\n",
    "    num_layers=NUM_LAYERS,\n",
    "    units=UNITS,\n",
    "    d_model=D_MODEL,\n",
    "    num_heads=NUM_HEADS,\n",
    "    dropout=DROPOUT)\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ìŠ=3\n"
     ]
    }
   ],
   "source": [
    "def loss_function(y_true, y_pred):\n",
    "  y_true = tf.reshape(y_true, shape=(-1, MAX_LENGTH - 1))\n",
    "  \n",
    "  loss = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "      from_logits=True, reduction='none')(y_true, y_pred)\n",
    "\n",
    "  mask = tf.cast(tf.not_equal(y_true, 0), tf.float32)\n",
    "  loss = tf.multiply(loss, mask)\n",
    "\n",
    "  return tf.reduce_mean(loss)\n",
    "print(\"ìŠ=3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ìŠ=3\n"
     ]
    }
   ],
   "source": [
    "class CustomSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
    "\n",
    "  def __init__(self, d_model, warmup_steps=4000):\n",
    "    super(CustomSchedule, self).__init__()\n",
    "\n",
    "    self.d_model = d_model\n",
    "    self.d_model = tf.cast(self.d_model, tf.float32)\n",
    "\n",
    "    self.warmup_steps = warmup_steps\n",
    "\n",
    "  def __call__(self, step):\n",
    "    step = tf.cast(step, tf.float32)\n",
    "    arg1 = tf.math.rsqrt(step)\n",
    "    arg2 = step * (self.warmup_steps**-1.5)\n",
    "\n",
    "    return tf.math.rsqrt(self.d_model) * tf.math.minimum(arg1, arg2)\n",
    "print(\"ìŠ=3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ìŠ=3\n"
     ]
    }
   ],
   "source": [
    "learning_rate = CustomSchedule(D_MODEL)\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(\n",
    "    learning_rate, beta_1=0.9, beta_2=0.98, epsilon=1e-9)\n",
    "\n",
    "def accuracy(y_true, y_pred):\n",
    "  y_true = tf.reshape(y_true, shape=(-1, MAX_LENGTH - 1))\n",
    "  return tf.keras.metrics.sparse_categorical_accuracy(y_true, y_pred)\n",
    "\n",
    "model.compile(optimizer=optimizer, loss=loss_function, metrics=[accuracy])\n",
    "print(\"ìŠ=3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-14 13:23:36.405056: I tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:606] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n",
      "2025-02-14 13:23:36.600641: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x7fdfe9774640 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2025-02-14 13:23:36.600689: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): NVIDIA GeForce RTX 3060, Compute Capability 8.6\n",
      "2025-02-14 13:23:36.610479: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:255] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2025-02-14 13:23:36.632646: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:432] Loaded cuDNN version 8600\n",
      "2025-02-14 13:23:36.704204: I tensorflow/tsl/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n",
      "2025-02-14 13:23:36.755409: I ./tensorflow/compiler/jit/device_compiler.h:186] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "122/122 [==============================] - 27s 143ms/step - loss: 1.3747 - accuracy: 0.0183\n",
      "Epoch 2/100\n",
      "122/122 [==============================] - 11s 89ms/step - loss: 1.2532 - accuracy: 0.0256\n",
      "Epoch 3/100\n",
      "122/122 [==============================] - 8s 63ms/step - loss: 1.1411 - accuracy: 0.0256\n",
      "Epoch 4/100\n",
      "122/122 [==============================] - 8s 67ms/step - loss: 1.0769 - accuracy: 0.0258\n",
      "Epoch 5/100\n",
      "122/122 [==============================] - 8s 63ms/step - loss: 1.0265 - accuracy: 0.0278\n",
      "Epoch 6/100\n",
      "122/122 [==============================] - 7s 61ms/step - loss: 0.9831 - accuracy: 0.0307\n",
      "Epoch 7/100\n",
      "122/122 [==============================] - 8s 64ms/step - loss: 0.9417 - accuracy: 0.0330\n",
      "Epoch 8/100\n",
      "122/122 [==============================] - 8s 65ms/step - loss: 0.9002 - accuracy: 0.0355\n",
      "Epoch 9/100\n",
      "122/122 [==============================] - 7s 61ms/step - loss: 0.8548 - accuracy: 0.0387\n",
      "Epoch 10/100\n",
      "122/122 [==============================] - 7s 60ms/step - loss: 0.8052 - accuracy: 0.0431\n",
      "Epoch 11/100\n",
      "122/122 [==============================] - 7s 59ms/step - loss: 0.7500 - accuracy: 0.0481\n",
      "Epoch 12/100\n",
      "122/122 [==============================] - 8s 62ms/step - loss: 0.6903 - accuracy: 0.0537\n",
      "Epoch 13/100\n",
      "122/122 [==============================] - 8s 63ms/step - loss: 0.6282 - accuracy: 0.0604\n",
      "Epoch 14/100\n",
      "122/122 [==============================] - 8s 66ms/step - loss: 0.5626 - accuracy: 0.0676\n",
      "Epoch 15/100\n",
      "122/122 [==============================] - 8s 62ms/step - loss: 0.4955 - accuracy: 0.0759\n",
      "Epoch 16/100\n",
      "122/122 [==============================] - 8s 66ms/step - loss: 0.4258 - accuracy: 0.0859\n",
      "Epoch 17/100\n",
      "122/122 [==============================] - 8s 66ms/step - loss: 0.3567 - accuracy: 0.0968\n",
      "Epoch 18/100\n",
      "122/122 [==============================] - 8s 66ms/step - loss: 0.2893 - accuracy: 0.1086\n",
      "Epoch 19/100\n",
      "122/122 [==============================] - 7s 60ms/step - loss: 0.2267 - accuracy: 0.1205\n",
      "Epoch 20/100\n",
      "122/122 [==============================] - 7s 60ms/step - loss: 0.1698 - accuracy: 0.1314\n",
      "Epoch 21/100\n",
      "122/122 [==============================] - 7s 61ms/step - loss: 0.1226 - accuracy: 0.1405\n",
      "Epoch 22/100\n",
      "122/122 [==============================] - 7s 61ms/step - loss: 0.0869 - accuracy: 0.1463\n",
      "Epoch 23/100\n",
      "122/122 [==============================] - 7s 60ms/step - loss: 0.0636 - accuracy: 0.1491\n",
      "Epoch 24/100\n",
      "122/122 [==============================] - 7s 59ms/step - loss: 0.0501 - accuracy: 0.1506\n",
      "Epoch 25/100\n",
      "122/122 [==============================] - 8s 66ms/step - loss: 0.0410 - accuracy: 0.1516\n",
      "Epoch 26/100\n",
      "122/122 [==============================] - 8s 64ms/step - loss: 0.0370 - accuracy: 0.1516\n",
      "Epoch 27/100\n",
      "122/122 [==============================] - 7s 61ms/step - loss: 0.0343 - accuracy: 0.1519\n",
      "Epoch 28/100\n",
      "122/122 [==============================] - 7s 60ms/step - loss: 0.0323 - accuracy: 0.1521\n",
      "Epoch 29/100\n",
      "122/122 [==============================] - 7s 61ms/step - loss: 0.0303 - accuracy: 0.1523\n",
      "Epoch 30/100\n",
      "122/122 [==============================] - 8s 65ms/step - loss: 0.0297 - accuracy: 0.1523\n",
      "Epoch 31/100\n",
      "122/122 [==============================] - 8s 65ms/step - loss: 0.0290 - accuracy: 0.1523\n",
      "Epoch 32/100\n",
      "122/122 [==============================] - 7s 60ms/step - loss: 0.0281 - accuracy: 0.1524\n",
      "Epoch 33/100\n",
      "122/122 [==============================] - 7s 59ms/step - loss: 0.0282 - accuracy: 0.1524\n",
      "Epoch 34/100\n",
      "122/122 [==============================] - 7s 61ms/step - loss: 0.0277 - accuracy: 0.1523\n",
      "Epoch 35/100\n",
      "122/122 [==============================] - 8s 62ms/step - loss: 0.0245 - accuracy: 0.1531\n",
      "Epoch 36/100\n",
      "122/122 [==============================] - 7s 60ms/step - loss: 0.0229 - accuracy: 0.1534\n",
      "Epoch 37/100\n",
      "122/122 [==============================] - 7s 58ms/step - loss: 0.0207 - accuracy: 0.1541\n",
      "Epoch 38/100\n",
      "122/122 [==============================] - 7s 58ms/step - loss: 0.0190 - accuracy: 0.1544\n",
      "Epoch 39/100\n",
      "122/122 [==============================] - 7s 61ms/step - loss: 0.0174 - accuracy: 0.1550\n",
      "Epoch 40/100\n",
      "122/122 [==============================] - 8s 64ms/step - loss: 0.0169 - accuracy: 0.1551\n",
      "Epoch 41/100\n",
      "122/122 [==============================] - 8s 63ms/step - loss: 0.0160 - accuracy: 0.1552\n",
      "Epoch 42/100\n",
      "122/122 [==============================] - 7s 61ms/step - loss: 0.0146 - accuracy: 0.1556\n",
      "Epoch 43/100\n",
      "122/122 [==============================] - 7s 60ms/step - loss: 0.0138 - accuracy: 0.1557\n",
      "Epoch 44/100\n",
      "122/122 [==============================] - 7s 59ms/step - loss: 0.0128 - accuracy: 0.1560\n",
      "Epoch 45/100\n",
      "122/122 [==============================] - 8s 62ms/step - loss: 0.0120 - accuracy: 0.1561\n",
      "Epoch 46/100\n",
      "122/122 [==============================] - 8s 63ms/step - loss: 0.0116 - accuracy: 0.1564\n",
      "Epoch 47/100\n",
      "122/122 [==============================] - 7s 61ms/step - loss: 0.0106 - accuracy: 0.1565\n",
      "Epoch 48/100\n",
      "122/122 [==============================] - 7s 60ms/step - loss: 0.0109 - accuracy: 0.1565\n",
      "Epoch 49/100\n",
      "122/122 [==============================] - 7s 59ms/step - loss: 0.0102 - accuracy: 0.1567\n",
      "Epoch 50/100\n",
      "122/122 [==============================] - 7s 59ms/step - loss: 0.0093 - accuracy: 0.1569\n",
      "Epoch 51/100\n",
      "122/122 [==============================] - 7s 59ms/step - loss: 0.0094 - accuracy: 0.1569\n",
      "Epoch 52/100\n",
      "122/122 [==============================] - 7s 59ms/step - loss: 0.0088 - accuracy: 0.1570\n",
      "Epoch 53/100\n",
      "122/122 [==============================] - 7s 58ms/step - loss: 0.0081 - accuracy: 0.1572\n",
      "Epoch 54/100\n",
      "122/122 [==============================] - 7s 58ms/step - loss: 0.0082 - accuracy: 0.1571\n",
      "Epoch 55/100\n",
      "122/122 [==============================] - 7s 60ms/step - loss: 0.0081 - accuracy: 0.1571\n",
      "Epoch 56/100\n",
      "122/122 [==============================] - 7s 59ms/step - loss: 0.0073 - accuracy: 0.1573\n",
      "Epoch 57/100\n",
      "122/122 [==============================] - 7s 60ms/step - loss: 0.0068 - accuracy: 0.1575\n",
      "Epoch 58/100\n",
      "122/122 [==============================] - 8s 66ms/step - loss: 0.0076 - accuracy: 0.1573\n",
      "Epoch 59/100\n",
      "122/122 [==============================] - 8s 64ms/step - loss: 0.0067 - accuracy: 0.1576\n",
      "Epoch 60/100\n",
      "122/122 [==============================] - 7s 60ms/step - loss: 0.0064 - accuracy: 0.1576\n",
      "Epoch 61/100\n",
      "122/122 [==============================] - 7s 59ms/step - loss: 0.0065 - accuracy: 0.1575\n",
      "Epoch 62/100\n",
      "122/122 [==============================] - 7s 60ms/step - loss: 0.0063 - accuracy: 0.1576\n",
      "Epoch 63/100\n",
      "122/122 [==============================] - 7s 59ms/step - loss: 0.0059 - accuracy: 0.1577\n",
      "Epoch 64/100\n",
      "122/122 [==============================] - 7s 61ms/step - loss: 0.0057 - accuracy: 0.1577\n",
      "Epoch 65/100\n",
      "122/122 [==============================] - 8s 61ms/step - loss: 0.0061 - accuracy: 0.1577\n",
      "Epoch 66/100\n",
      "122/122 [==============================] - 7s 57ms/step - loss: 0.0055 - accuracy: 0.1577\n",
      "Epoch 67/100\n",
      "122/122 [==============================] - 7s 57ms/step - loss: 0.0054 - accuracy: 0.1578\n",
      "Epoch 68/100\n",
      "122/122 [==============================] - 7s 58ms/step - loss: 0.0047 - accuracy: 0.1579\n",
      "Epoch 69/100\n",
      "122/122 [==============================] - 7s 57ms/step - loss: 0.0051 - accuracy: 0.1579\n",
      "Epoch 70/100\n",
      "122/122 [==============================] - 7s 59ms/step - loss: 0.0049 - accuracy: 0.1579\n",
      "Epoch 71/100\n",
      "122/122 [==============================] - 7s 59ms/step - loss: 0.0046 - accuracy: 0.1580\n",
      "Epoch 72/100\n",
      "122/122 [==============================] - 8s 62ms/step - loss: 0.0048 - accuracy: 0.1579\n",
      "Epoch 73/100\n",
      "122/122 [==============================] - 8s 62ms/step - loss: 0.0043 - accuracy: 0.1581\n",
      "Epoch 74/100\n",
      "122/122 [==============================] - 8s 63ms/step - loss: 0.0045 - accuracy: 0.1580\n",
      "Epoch 75/100\n",
      "122/122 [==============================] - 7s 58ms/step - loss: 0.0041 - accuracy: 0.1580\n",
      "Epoch 76/100\n",
      "122/122 [==============================] - 7s 58ms/step - loss: 0.0040 - accuracy: 0.1580\n",
      "Epoch 77/100\n",
      "122/122 [==============================] - 7s 58ms/step - loss: 0.0044 - accuracy: 0.1580\n",
      "Epoch 78/100\n",
      "122/122 [==============================] - 7s 57ms/step - loss: 0.0040 - accuracy: 0.1581\n",
      "Epoch 79/100\n",
      "122/122 [==============================] - 7s 59ms/step - loss: 0.0038 - accuracy: 0.1581\n",
      "Epoch 80/100\n",
      "122/122 [==============================] - 8s 62ms/step - loss: 0.0038 - accuracy: 0.1581\n",
      "Epoch 81/100\n",
      "122/122 [==============================] - 7s 61ms/step - loss: 0.0039 - accuracy: 0.1580\n",
      "Epoch 82/100\n",
      "122/122 [==============================] - 7s 58ms/step - loss: 0.0039 - accuracy: 0.1580\n",
      "Epoch 83/100\n",
      "122/122 [==============================] - 7s 58ms/step - loss: 0.0037 - accuracy: 0.1581\n",
      "Epoch 84/100\n",
      "122/122 [==============================] - 7s 59ms/step - loss: 0.0035 - accuracy: 0.1581\n",
      "Epoch 85/100\n",
      "122/122 [==============================] - 7s 61ms/step - loss: 0.0037 - accuracy: 0.1582\n",
      "Epoch 86/100\n",
      "122/122 [==============================] - 7s 61ms/step - loss: 0.0035 - accuracy: 0.1581\n",
      "Epoch 87/100\n",
      "122/122 [==============================] - 7s 60ms/step - loss: 0.0036 - accuracy: 0.1581\n",
      "Epoch 88/100\n",
      "122/122 [==============================] - 8s 63ms/step - loss: 0.0032 - accuracy: 0.1582\n",
      "Epoch 89/100\n",
      "122/122 [==============================] - 7s 59ms/step - loss: 0.0033 - accuracy: 0.1582\n",
      "Epoch 90/100\n",
      "122/122 [==============================] - 7s 58ms/step - loss: 0.0031 - accuracy: 0.1582\n",
      "Epoch 91/100\n",
      "122/122 [==============================] - 7s 59ms/step - loss: 0.0031 - accuracy: 0.1583\n",
      "Epoch 92/100\n",
      "122/122 [==============================] - 7s 60ms/step - loss: 0.0029 - accuracy: 0.1582\n",
      "Epoch 93/100\n",
      "122/122 [==============================] - 7s 59ms/step - loss: 0.0029 - accuracy: 0.1582\n",
      "Epoch 94/100\n",
      "122/122 [==============================] - 7s 57ms/step - loss: 0.0027 - accuracy: 0.1583\n",
      "Epoch 95/100\n",
      "122/122 [==============================] - 7s 58ms/step - loss: 0.0027 - accuracy: 0.1583\n",
      "Epoch 96/100\n",
      "122/122 [==============================] - 7s 58ms/step - loss: 0.0027 - accuracy: 0.1583\n",
      "Epoch 97/100\n",
      "122/122 [==============================] - 7s 58ms/step - loss: 0.0029 - accuracy: 0.1583\n",
      "Epoch 98/100\n",
      "122/122 [==============================] - 7s 59ms/step - loss: 0.0026 - accuracy: 0.1583\n",
      "Epoch 99/100\n",
      "122/122 [==============================] - 7s 61ms/step - loss: 0.0027 - accuracy: 0.1583\n",
      "Epoch 100/100\n",
      "122/122 [==============================] - 8s 62ms/step - loss: 0.0028 - accuracy: 0.1583\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 100\n",
    "history = model.fit(dataset, epochs=EPOCHS, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 5. ëª¨ë¸ í‰ê°€í•˜ê¸°\n",
    "\n",
    "Step 1ì—ì„œ ì„ íƒí•œ ì „ì²˜ë¦¬ ë°©ë²•ì„ ê³ ë ¤í•˜ì—¬ ì…ë ¥ëœ ë¬¸ì¥ì— ëŒ€í•´ì„œ ëŒ€ë‹µì„ ì–»ëŠ” ì˜ˆì¸¡ í•¨ìˆ˜ë¥¼ ë§Œë“­ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ìŠ=3\n"
     ]
    }
   ],
   "source": [
    "def decoder_inference(sentence):\n",
    "  sentence = preprocess_korean_sentence(sentence)\n",
    "\n",
    "  # ì…ë ¥ëœ ë¬¸ì¥ì„ ì •ìˆ˜ ì¸ì½”ë”© í›„, ì‹œì‘ í† í°ê³¼ ì¢…ë£Œ í† í°ì„ ì•ë’¤ë¡œ ì¶”ê°€.\n",
    "  # ex) Where have you been? â†’ [[8331   86   30    5 1059    7 8332]]\n",
    "  sentence = tf.expand_dims(\n",
    "      START_TOKEN + tokenizer.encode(sentence) + END_TOKEN, axis=0)\n",
    "\n",
    "  # ë””ì½”ë”ì˜ í˜„ì¬ê¹Œì§€ì˜ ì˜ˆì¸¡í•œ ì¶œë ¥ ì‹œí€€ìŠ¤ê°€ ì§€ì†ì ìœ¼ë¡œ ì €ì¥ë˜ëŠ” ë³€ìˆ˜.\n",
    "  # ì²˜ìŒì—ëŠ” ì˜ˆì¸¡í•œ ë‚´ìš©ì´ ì—†ìŒìœ¼ë¡œ ì‹œì‘ í† í°ë§Œ ë³„ë„ ì €ì¥. ex) 8331\n",
    "  output_sequence = tf.expand_dims(START_TOKEN, 0)\n",
    "\n",
    "  # ë””ì½”ë”ì˜ ì¸í¼ëŸ°ìŠ¤ ë‹¨ê³„\n",
    "  for i in range(MAX_LENGTH):\n",
    "    # ë””ì½”ë”ëŠ” ìµœëŒ€ MAX_LENGTHì˜ ê¸¸ì´ë§Œí¼ ë‹¤ìŒ ë‹¨ì–´ ì˜ˆì¸¡ì„ ë°˜ë³µí•©ë‹ˆë‹¤.\n",
    "    predictions = model(inputs=[sentence, output_sequence], training=False)\n",
    "    predictions = predictions[:, -1:, :]\n",
    "\n",
    "    # í˜„ì¬ ì˜ˆì¸¡í•œ ë‹¨ì–´ì˜ ì •ìˆ˜\n",
    "    predicted_id = tf.cast(tf.argmax(predictions, axis=-1), tf.int32)\n",
    "\n",
    "    # ë§Œì•½ í˜„ì¬ ì˜ˆì¸¡í•œ ë‹¨ì–´ê°€ ì¢…ë£Œ í† í°ì´ë¼ë©´ forë¬¸ì„ ì¢…ë£Œ\n",
    "    if tf.equal(predicted_id, END_TOKEN[0]):\n",
    "      break\n",
    "\n",
    "    # ì˜ˆì¸¡í•œ ë‹¨ì–´ë“¤ì€ ì§€ì†ì ìœ¼ë¡œ output_sequenceì— ì¶”ê°€ë©ë‹ˆë‹¤.\n",
    "    # ì´ output_sequenceëŠ” ë‹¤ì‹œ ë””ì½”ë”ì˜ ì…ë ¥ì´ ë©ë‹ˆë‹¤.\n",
    "    output_sequence = tf.concat([output_sequence, predicted_id], axis=-1)\n",
    "\n",
    "  return tf.squeeze(output_sequence, axis=0)\n",
    "print(\"ìŠ=3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ìŠ=3\n"
     ]
    }
   ],
   "source": [
    "def sentence_generation(sentence):\n",
    "  # ì…ë ¥ ë¬¸ì¥ì— ëŒ€í•´ì„œ ë””ì½”ë”ë¥¼ ë™ì‘ ì‹œì¼œ ì˜ˆì¸¡ëœ ì •ìˆ˜ ì‹œí€€ìŠ¤ë¥¼ ë¦¬í„´ë°›ìŠµë‹ˆë‹¤.\n",
    "  prediction = decoder_inference(sentence)\n",
    "\n",
    "  # ì •ìˆ˜ ì‹œí€€ìŠ¤ë¥¼ ë‹¤ì‹œ í…ìŠ¤íŠ¸ ì‹œí€€ìŠ¤ë¡œ ë³€í™˜í•©ë‹ˆë‹¤.\n",
    "  predicted_sentence = tokenizer.decode(\n",
    "      [i for i in prediction if i < tokenizer.vocab_size])\n",
    "\n",
    "  print('ì…ë ¥ : {}'.format(sentence))\n",
    "  print('ì¶œë ¥ : {}'.format(predicted_sentence))\n",
    "\n",
    "  return predicted_sentence\n",
    "print(\"ìŠ=3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ìµœì¢… ê²°ê³¼"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ì…ë ¥ : ì˜¤ëŠ˜ ë‚ ì”¨ ì–´ë•Œ?\n",
      "ì¶œë ¥ : ë‚ ì”¨ ì–´í”Œì— ë¬¼ì–´ë³´ì„¸ìš”\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'ë‚ ì”¨ ì–´í”Œì— ë¬¼ì–´ë³´ì„¸ìš”'"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence_generation('ì˜¤ëŠ˜ ë‚ ì”¨ ì–´ë•Œ?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ì…ë ¥ : ë°°ê°€ ë„ˆë¬´ ì•„íŒŒ\n",
      "ì¶œë ¥ : ì¢€ ì‰¬ì„¸ìš”\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'ì¢€ ì‰¬ì„¸ìš”'"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence_generation('ë°°ê°€ ë„ˆë¬´ ì•„íŒŒ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### íšŒê³ \n",
    "1. accuracyê°€ 0.1583ìœ¼ë¡œ í•™ìŠµì´ ë§ˆë¬´ë¦¬ ë˜ì—ˆëŠ”ë° ìµœì¢… ê²°ê³¼ë¥¼ í™•ì¸í•˜ë©´ ì„±ëŠ¥ì´ ë„ˆë¬´ ì¢‹ê²Œ ë‚˜ì™€ì„œ sparse categorical accuracyê°€ ì–´ë–»ê²Œ ì¸¡ì •ë˜ëŠ”ì§€ ì•Œì•„ë³´ì•˜ë‹¤.\n",
    "    - y_predëŠ” modelì˜ ìµœì¢… ì¶œë ¥ìœ¼ë¡œ (batch_size, seq_len, vocab_size)ì´ë‹¤.\n",
    "    - sparse categorical lossì— y_trueì™€ y_predê°€ ë§¤ê°œë³€ìˆ˜ë¡œ ë“¤ì–´ê°€ê³  y_predëŠ” argmaxê°€ ì ìš©ëœë‹¤ê³  í•œë‹¤.\n",
    "    - ê·¸ë ‡ë‹¤ë©´ accuracyëŠ” ì˜ˆì¸¡ í† í° ë¼ë¦¬ì˜ ì •í™•ë„ë¥¼ ì˜ë¯¸í•œë‹¤.\n",
    "    - ê²°ë¡ ì ìœ¼ë¡œ ìƒì„± ëª¨ë¸ì—ì„œ accuracyë³´ë‹¤ ì •ì„±í‰ê°€ê°€ ë” ì¤‘ìš”í•˜ë‹¤ëŠ”ê±¸ ì •ë§ ë§ì´ ëŠë¼ê²Œ ë˜ì—ˆë‹¤.\n",
    "\n",
    "2. ì „ì²´ì ì¸ í”„ë¡œì íŠ¸ ìš”êµ¬ì‚¬í•­ë³´ë‹¤ ë¨¼ì € í•œêµ­ì–´ í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬ì— ëŒ€í•´ ê³ ë¯¼í•  ë•Œ, vocab í¬ê¸°ê°€ ì»¤ì§€ë”ë¼ë„ ë¹ˆë„ ìˆ˜ë¥¼ ê³ ë ¤í•´ì„œ í† í°ì„ ì¤„ì´ì§€ ë§ì•„ì•¼ê² ë‹¤ëŠ” ê²°ë¡ ì„ ë‚´ë ¸ì—ˆë‹¤. ë˜í•œ í”„ë¡œì íŠ¸ì˜ ìš”êµ¬ì‚¬í•­ì— ë”°ë¼ ì „ì²˜ë¦¬ë¥¼ ë”°ë¼ê°€ë„ ë³´ë‹ˆ ì–´ë–¤ í† í°ì„ ëª…ì‹œì ìœ¼ë¡œ ì‚­ì œí•˜ì§€ ì•Šê³  vocab í¬ê¸°ë¥¼ ë¨¼ì € ê²°ì •ì§“ëŠ” ë°©ì‹ì˜ í† í°í™”ë¥¼ ì§„í–‰í•˜ê²Œ ë˜ì—ˆë‹¤. í•´ë‹¹ í† í°í™” ë°©ì‹ì´ ì–´ë–»ê²Œ ì´ë£¨ì–´ì§€ëŠ”ì§€ ë” ìì„¸íˆ ì‚´í´ë³´ì•˜ë‹¤.\n",
    "    - ì„œë¸Œì›Œë“œ í† í°í™” ì›ë¦¬\n",
    "    - ì˜ˆ. birdstrike => birdì™€ strikeì˜ ì„œë¸Œì›Œë“œë¡œ ë¶„ë¦¬\n",
    "    - ì£¼ì–´ì§„ ë§ë­‰ì¹˜(corpus; í›ˆë ¨ë°ì´í„°ì…‹)ì—ì„œ ê°€ì¥ ìì£¼ ë“±ì¥í•˜ëŠ” ì„œë¸Œì›Œë“œ ë‹¨ìœ„ë¥¼ ì°¾ì•„ vocabë¥¼ êµ¬ì¶•í•œë‹¤.\n",
    "    - Greedy í† í°í™”; ë¬¸ì¥ì„ í† í°í™”í•  ë•Œ, ê°€ì¥ ê¸´ ë§¤ì¹­ ì„œë¸Œì›Œë“œë¶€í„° ì‹œì‘í•˜ì—¬ ì ì§„ì ìœ¼ë¡œ ì‘ì€ ë‹¨ìœ„ë¡œ ë¶„í• í•œë‹¤.\n",
    "    - UNKOWN ì²˜ë¦¬; ì–´íœ˜ì— ì—†ëŠ” ë‹¨ì–´ëŠ” ë” ì‘ì€ ì„œë¸Œì›Œë“œ ë‹¨ìœ„ë¡œ ë¶„í•´ëœë‹¤. ê°œë³„ ë¬¸ì ë‹¨ìœ„ê¹Œì§€ë„ ë¶„í•´í•  ìˆ˜ ìˆë‹¤.\n",
    "    - í˜•íƒœí•™ì ìœ¼ë¡œ ë³µì¡í•œ ì–¸ì–´ë‚˜ ì‹ ì¡°ì–´ê°€ ë§ì€ ë„ë©”ì¸ì— ìœ ìš©í•˜ë‹¤\n",
    "    - ê²°êµ­ vocabì„ êµ¬ì„±í•˜ê²Œ ë˜ëŠ” ë°©ì‹ì€ í•­ìƒ í† í°ì˜ ë“±ì¥ ë¹ˆë„ìˆ˜ì— ë”°ë¼ ê²°ì •ë˜ëŠ” ê²ƒ ì²˜ëŸ¼ ëŠê»´ì§„ë‹¤. ê³¼ì—° í•­ìƒ ê·¸ëŸ´ê¹Œ?\n",
    "\n",
    "3. ì´ ë‚´ìš©ì€ Transformer êµ¬ì¡°ë¥¼ ì„¸ì„¸í•˜ê²Œ ëœ¯ì–´ ë³¼ë•Œ ìƒê²¼ë˜ ì˜ë¬¸ ì‚¬í•­ê³¼ ê²°ë¡ ìœ¼ë¡œ ë‹¤ìŒì— í—·ê°ˆë¦¬ì§€ ì•Šê¸° ìœ„í•´ ê¸°ë¡í•´ë†“ìœ¼ë ¤ í•œë‹¤.\n",
    "    - tensorë‚˜ matrix ë“± ì¶•ì´ 3ê°œ ì´ìƒìœ¼ë¡œ ë§ì•„ì§ˆ ë•Œ, Transposeë¥¼ ì ìš©í•˜ë©´ ë§¨ ë§ˆì§€ë§‰ 2 ì¶•ì—ë§Œ ì ìš©ëœë‹¤.\n",
    "    - LSTMì´ë‚˜ RNNì—ì„œ ìœ ì‚¬í•œ íƒœìŠ¤í¬ì— ëŒ€í•´ í•™ìŠµí•  ë•Œ, ìƒê°í•´ë³´ë©´ padding mask ë¼ëŠ”ê±¸ ì‚¬ìš©í•˜ì§€ ì•Šì€ ê²ƒì„ ì•Œ ìˆ˜ ìˆë‹¤. ì™œëƒí•˜ë©´ ì…ë ¥ sequenceì—ì„œ íŒ¨ë”©ì€ 0ìœ¼ë¡œ ì…ë ¥í•˜ê¸° ë•Œë¬¸ì— ë³µì¡í•œ ë„¤íŠ¸ì›Œí¬ë¥¼ ì§€ë‚˜ë”ë¼ë„ 0ì´ ì¶œë ¥ë˜ì–´ ê³„ì‚°ì— ë°˜ì˜ì´ ì•ˆë˜ê¸° ë•Œë¬¸ì´ë‹¤. í•˜ì§€ë§Œ Transformerì—ì„œëŠ” padding maskë¥¼ ì‚¬ìš©í•˜ëŠ”ë° ê·¸ ì´ìœ ëŠ” ë˜‘ê°™ì´ 0ìœ¼ë¡œ ì¶œë ¥ë˜ì§€ë§Œ attention_weight(ì–´í…ì…˜ ì ìˆ˜)ë¥¼ ê³„ì‚°í•  ë•Œì—ëŠ” Softmax í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•˜ê¸° ë•Œë¬¸ì—, 'e^0'ì€ '1'ì´ ë˜ì–´ íŒ¨ë”©ì´ ê³„ì‚°ì— ê´€ì—¬í•˜ê²Œ ë˜ëŠ” ë¬¸ì œê°€ ìƒê¸´ë‹¤. ë”°ë¼ì„œ ì–´ë–¤ ìœ„ì¹˜ì— íŒ¨ë”©ì´ ë“¤ì–´ê°”ëŠ”ì§€ ê¸°ì–µí•˜ê³  í•´ë‹¹ ìœ„ì¹˜ì— ë§¤ìš° í° ìŒìˆ˜ë¥¼ ê³±í•´ì„œ 0ì— ê°€ê¹ê²Œ ë§Œë“¤ì–´ ì¤˜ì•¼ í•œë‹¤. ì´ëŸ¬í•œ ì´ìœ ë¡œ padding maskê°€ í•„ìš”í•˜ë‹¤.\n",
    "    - [í¬ì§€ì…”ë„ ì¸ì½”ë”©](https://medium.com/thedeephub/positional-encoding-explained-a-deep-dive-into-transformer-pe-65cfe8cfe10b)ì—ì„œ ë’¤ë¡œ ê°ˆìˆ˜ë¡ ê°’ì˜ ë³€í™”ê°€ ì—†ê³  ì•ìª½ìœ¼ë¡œ ì˜¬ìˆ˜ë¡ ê°’ì˜ ë³€í™”ê°€ ì¦ì€ ì´ìœ ëŠ” pos/var ê°€ ì£¼ê¸° í•¨ìˆ˜ì˜ ë³€ìˆ˜ë¡œ ë“¤ì–´ê°€ëŠ”ë° ë’¤ë¡œ ê°ˆìˆ˜ë¡ varì˜ í¬ê¸°ê°€ ì»¤ì ¸ ì£¼ê¸°ê°€ ëŠ˜ì–´ë‚˜ëŠ” íš¨ê³¼ê°€ ë‚˜íƒ€ë‚˜ê¸° ë•Œë¬¸ì´ë‹¤. ê·¸ëŸ¼ ì™œ ì´ë ‡ê²Œ êµ¬í˜„í–ˆì„ê¹Œ? ë…¼ë¬¸ì˜ ì €ìëŠ” \"ëª¨ë¸ì´ ìƒëŒ€ì  ìœ„ì¹˜ì— ë”°ë¼ ì£¼ì˜ë¥¼ ê¸°ìš¸ì´ëŠ” ë²•ì„ ì‰½ê²Œ í•™ìŠµí•  ìˆ˜ ìˆì„ ê²ƒ\"ì´ë¼ëŠ” ê°€ì„¤ì„ ì„¸ì› ìœ¼ë©° ì´ì— ë”°ë¼ ê³ ì •ëœ ì˜¤í”„ì…‹kì— ëŒ€í•´ PE(pos+k)ëŠ” PE(pos)ì˜ ì„ í˜• í•¨ìˆ˜ë¡œ í‘œí˜„ë  ìˆ˜ ìˆë‹¤ë¼ê³  í•˜ì˜€ë‹¤. ì´ ë§ì€ PE(pos)ê°€ PE(pos+K)ë¡œ ì´ë™í•  ìˆ˜ ìˆê²Œ ë§Œë“œëŠ” í–‰ë ¬ Mì´ ì¡´ì¬í•˜ê³  ì´ í–‰ë ¬ Mì€ posì™€ ë…ë¦½ì ì´ë¼ëŠ” ì˜ë¯¸ë¼ê³  í•œë‹¤. ì¦‰, ì‹œì‘í•˜ëŠ” ì ˆëŒ€ ìœ„ì¹˜ì™€ ê´€ê³„ì—†ì´ ê°™ì€ ë°©ì‹(posê°€ ë³€í™”í•˜ì—¬ë„)ìœ¼ë¡œ ì‘ë™í•œë‹¤ëŠ” ëœ»ì´ë‹¤. ë”°ë¼ì„œ ì ˆëŒ€ì ì¸ ìœ„ì¹˜ ì •ë³´ë¥¼ ì‚¬ìš©í•˜ì§€ ì•Šê³ ë„ ìƒëŒ€ì ì¸ ìœ„ì¹˜ ì •ë³´ë¥¼ í•™ìŠµí•  ìˆ˜ ìˆë„ë¡ í•œë‹¤ëŠ” ì˜ë¯¸ì´ë‹¤. ì˜ˆ. 'not'ì´ ë¬¸ì¥ì—ì„œ ì–´ë””ì— ë‚˜íƒ€ë‚˜ëŠ”ì§€ì— ê´€ê³„ ì—†ì´ ê·¼ì²˜ ë‹¨ì–´ì˜ ì˜ë¯¸ë¥¼ ìˆ˜ì •í•œë‹¤ëŠ” ê²ƒì„ í•™ìŠµí•  ìˆ˜ ìˆë‹¤ëŠ” ì˜ë¯¸."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "GPU",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
